{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "#print(\"ord of a: \",ord('a')-97+1)\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from above:\n",
    "- ' ' = 0\n",
    "- a = 1\n",
    "- b = 2\n",
    "- etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interupt:  (1, 64, 27)\n",
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "interupt:  (1, 64, 27)\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "interupt:  (1, 1, 27)\n",
      "[' a']\n",
      "interupt:  (1, 1, 27)\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      ##print(\"interupt: \",self._text[self._cursor[b]])\n",
    "      ##print(\"self cursor: \",self._cursor)\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size      \n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    #print(\"interupt: \",np.array(batches).shape)\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    #print(\"interupt: \",np.array(batches).shape)\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    #print(\"interupt: \",s)\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "questions\n",
    "\n",
    "- logprob - why divide by labels.shape i.e. the number of letters\n",
    "- sample distribution makes some sense if np.sum(distribution) = 1\n",
    "- - the larger a probability the higher the liklihood it will push s over r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 200\n",
    "time0 = time()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "  print(\"model run time: %0.3fs\"%(time()-time0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ### all gates\n",
    "  gx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes*4], -0.1, 0.1))\n",
    "  gm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  gb = tf.Variable(tf.zeros([1, num_nodes*4]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  #fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  #fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  #fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  #cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  #cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  #cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  #ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  #om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  #ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    #### one gate input,forget,update,ouput\n",
    "    gates = tf.matmul(i,gx)+tf.matmul(o,gm)+gb\n",
    "    #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    #state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    state = tf.sigmoid(gates[:,64:128])*state + tf.sigmoid(gates[:,:64])*tf.tanh(gates[:,128:192])\n",
    "    return tf.sigmoid(gates[:,192:]) * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 200\n",
    "time0 = time()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "  print(\"model run time: %0.3fs\"%(time()-time0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch2posit(batch):\n",
    "  new_batch =[]  \n",
    "  [new_batch.append(argmax(x)) for x in batch]\n",
    "  return new_batch\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ### all gates\n",
    "  gx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes*4], -0.1, 0.1))\n",
    "  gm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  gb = tf.Variable(tf.zeros([1, num_nodes*4]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  \n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    #### one gate input,forget,update,ouput: each 64 in size\n",
    "    #embed = tf.nn.embedding_lookup(embeddings,batch2posit(i))\n",
    "    #input_data = batch2posit(i)\n",
    "    embed = tf.nn.embedding_lookup(embeddings,tf.argmax(i,axis=1))\n",
    "    gates = tf.matmul(embed,gx)+tf.matmul(o,gm)+gb\n",
    "    state = tf.sigmoid(gates[:,64:128])*state + tf.sigmoid(gates[:,:64])*tf.tanh(gates[:,128:192])\n",
    "    return tf.sigmoid(gates[:,192:]) * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 200\n",
    "time0 = time()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "  print(\"model run time: %0.3fs\"%(time()-time0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2 part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "#print(\"ord of a: \",ord('a')-97+1)\n",
    "\n",
    "def bigram2id(bigram):\n",
    "  return 27*char2id(bigram[0])+char2id(bigram[1])\n",
    "  \n",
    "def id2bigram(dictid):\n",
    "  first = dictid//27\n",
    "  second = dictid%27\n",
    "  return id2char(first)+id2char(second)\n",
    "\n",
    "print(bigram2id('ab'), bigram2id('zg'), bigram2id(' f'), bigram2id('ïg'))\n",
    "print(id2bigram(2), id2bigram(526), id2bigram(728))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=5\n",
    "vocabulary_size = 729\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      ###print(\"self._cursor:\",self._cursor)\n",
    "      ###print(\"b:\", b)\n",
    "      ###print(self._text[self._cursor[b]]+self._text[self._cursor[b]+1])\n",
    "      ###print(bigram2id(self._text[self._cursor[b]]+self._text[self._cursor[b]+1]))\n",
    "      ###print(\"cursor[b+1]:\",self._text[self._cursor[(b+1)% self._text_size]])\n",
    "      batch[b, bigram2id(self._text[self._cursor[b]]+self._text[self._cursor[b]+1])] = 1.0\n",
    "      #print(\"interupt: \",self._text[self._cursor[b]])\n",
    "      self._cursor[b] = (self._cursor[b] + 2) % self._text_size      \n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    #print(\"interupt: \",np.array(batches).shape)\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    #print(\"interupt: \",np.array(batches).shape)\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2bigram(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    #print(\"interupt: \",s)\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ### all gates\n",
    "  gx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes*4], -0.1, 0.1))\n",
    "  gm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  gb = tf.Variable(tf.zeros([1, num_nodes*4]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  \n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    #embed = tf.nn.embedding_lookup(embeddings,batch2posit(i))\n",
    "    #input_data = batch2posit(i)\n",
    "    embed = tf.nn.embedding_lookup(embeddings,tf.argmax(i,axis=1))\n",
    "    gates = tf.matmul(embed,gx)+tf.matmul(o,gm)+gb\n",
    "    state = tf.sigmoid(gates[:,64:128])*state + tf.sigmoid(gates[:,:64])*tf.tanh(gates[:,128:192])\n",
    "    return tf.sigmoid(gates[:,192:]) * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 200\n",
    "time0 = time()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "  print(\"model run time: %0.3fs\"%(time()-time0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3 part 3: implimenting dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "keep_prob = 0.5    \n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ### all gates\n",
    "  gx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes*4], -0.1, 0.1))\n",
    "  gm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  gb = tf.Variable(tf.zeros([1, num_nodes*4]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  \n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    #embed = tf.nn.embedding_lookup(embeddings,batch2posit(i))\n",
    "    #input_data = batch2posit(i)\n",
    "    embed = tf.nn.embedding_lookup(embeddings,tf.argmax(i,axis=1))\n",
    "    embed_dropout = tf.nn.dropout(embed,keep_prob)\n",
    "    gates = tf.matmul(embed,gx)+tf.matmul(o,gm)+gb\n",
    "    state = tf.sigmoid(gates[:,64:128])*state + tf.sigmoid(gates[:,:64])*tf.tanh(gates[:,128:192])\n",
    "    return tf.nn.dropout(tf.sigmoid(gates[:,192:]),keep_prob) * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 200\n",
    "time0 = time()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "  print(\"model run time: %0.3fs\"%(time()-time0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarch', 'when milit', 'lleria arc', ' abbeys an', 'married ur', 'hel and ri', 'y and litu', 'ay opened ', 'tion from ', 'migration ', 'new york o', 'he boeing ', 'e listed w', 'eber has p', 'o be made ', 'yer who re', 'ore signif', 'a fierce c', ' two six e', 'aristotle ', 'ity can be', ' and intra', 'tion of th', 'dy to pass', 'f certain ', 'at it will', 'e convince', 'ent told h', 'ampaign an', 'rver side ', 'ious texts', 'o capitali', 'a duplicat', 'gh ann es ', 'ine januar', 'ross zero ', 'cal theori', 'ast instan', ' dimension', 'most holy ', 't s suppor', 'u is still', 'e oscillat', 'o eight su', 'of italy l', 's the towe', 'klahoma pr', 'erprise li', 'ws becomes', 'et in a na', 'the fabian', 'etchy to r', ' sharman n', 'ised emper', 'ting in po', 'd neo lati', 'th risky r', 'encycloped', 'fense the ', 'duating fr', 'treet grid', 'ations mor', 'appeal of ', 'si have ma']\n",
      "['hists advo', 'tary gover', 'ches natio', 'nd monaste', 'rraca prin', 'ichard bae', 'urgical la', ' for passe', ' the natio', ' took plac', 'other well', ' seven six', 'with a glo', 'probably b', ' to recogn', 'eceived th', 'ficant tha', 'critic of ', 'eight in s', ' s uncause', 'e lost as ', 'acellular ', 'he size of', 's him a st', ' drugs con', 'l take to ', 'e the prie', 'him to nam', 'nd barred ', ' standard ', 's such as ', 'ize on the', 'te of the ', ' d hiver o', 'ry eight m', ' the lead ', 'ies classi', 'nce the no', 'nal analys', ' mormons b', 'rt or at l', 'l disagree', 'ting syste', 'ubtypes ba', 'languages ', 'er commiss', 'ress one n', 'inux suse ', 's the firs', 'azi concen', 'n society ', 'relatively', 'networks s', 'ror hirohi', 'olitical i', 'in most of', 'riskerdoo ', 'dic overvi', ' air compo', 'rom acnm a', 'd centerli', 're than an', ' devotiona', 'ade such d']\n",
      "[' ']\n",
      "[' ']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      ##print(\"interupt: \",self._text[self._cursor[b]])\n",
    "      ##print(\"self cursor: \",self._cursor)\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size      \n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    ##print(\"interupt: \",np.array(batches).shape)\n",
    "    for step in range(self._num_unrollings-1):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    #print(\"interupt: \",np.array(batches).shape)\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    #print(\"interupt: \",s)\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "vocabulary_size = 27\n",
    "num_nodes_1 = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():    \n",
    "\n",
    "\t#Variables\n",
    "\tgx = tf.Variable(tf.truncated_normal([embedding_size,num_nodes_1*4]))\n",
    "\tgb = tf.Variable(tf.truncated_normal([1,num_nodes_1*4]))\n",
    "\n",
    "\tgx_2 = tf.Variable(tf.truncated_normal([num_nodes_1,num_nodes_1*4]))\n",
    "\tgb_2 = tf.Variable(tf.truncated_normal([1,num_nodes_1*4]))\n",
    "\n",
    "\tw = tf.Variable(tf.truncated_normal([num_nodes_1,vocabulary_size]))\n",
    "\tb = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "\tembeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size],-1.0,1.0))\n",
    "\t\n",
    "\t#Placeholders\n",
    "\ttrain_data = list()\n",
    "\tfor _ in range(num_unrollings):\n",
    "\t\ttrain_data.append(tf.placeholder(tf.int32,shape = [batch_size,vocabulary_size]))\n",
    "\t##train_input = train_data[:num_unrollings]\n",
    "\t##train_labels = train_data[1:]\n",
    "\n",
    "\t#Constants #to be placeholders later?\n",
    "\tsaved_output_1 = tf.constant(np.zeros([batch_size,num_nodes_1],dtype = np.float32))\n",
    "\tsaved_state_1 = tf.constant(np.zeros([batch_size,num_nodes_1],dtype = np.float32))\n",
    "\n",
    "\t#lstm\n",
    "\tdef lstm_cell_1(i,state):\n",
    "\t\tembed = tf.nn.embedding_lookup(embeddings,tf.argmax(i,axis=1))\n",
    "\t\tgates = tf.matmul(embed,gx)+gb\n",
    "\t\tstate = tf.sigmoid(gates[:,num_nodes_1:num_nodes_1*2]\n",
    "\t\t\t)*state+tf.sigmoid(gates[:,:num_nodes_1])*tf.tanh(gates[:,num_nodes_1*2:num_nodes_1*3])\n",
    "\t\treturn tf.sigmoid(gates[:,num_nodes_1*3:])*tf.tanh(state),state\n",
    "\n",
    "\tdef lstm_cell_2(o,state):\n",
    "\t\t#embed = tf.nn.embedding_lookup(embeddings,tf.argmax(i,axis=1))\n",
    "\t\tgates = tf.matmul(o,gx_2)+gb_2\n",
    "\t\tstate = tf.sigmoid(gates[:,num_nodes_1:num_nodes_1*2]\n",
    "\t\t\t)*state+tf.sigmoid(gates[:,:num_nodes_1])*tf.tanh(gates[:,num_nodes_1*2:num_nodes_1*3])\n",
    "\t\treturn tf.sigmoid(gates[:,num_nodes_1*3:])*tf.tanh(state),state\n",
    "\n",
    "\t#running through lstm_1\t\n",
    "\tstate = saved_state_1\n",
    "\tfor i in reversed(train_data):\n",
    "\t\t_,state = lstm_cell_1(i,state)\n",
    "\t\t#outputs.append(output)\n",
    "\n",
    "\t#running through lstm_2\n",
    "\toutputs = list()\n",
    "\toutput = saved_output_1\n",
    "\tfor i in range(num_unrollings):\n",
    "\t\toutput,state = lstm_cell_2(output,state)\n",
    "\t\toutputs.append(output)\n",
    "\n",
    "\t#classifier\n",
    "\tlogits = tf.nn.xw_plus_b(tf.concat(outputs,0),w,b)\n",
    "\tloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "\t\tlabels = tf.concat(train_data,0), logits = logits))\n",
    "\n",
    "\t#optimizer\n",
    "\tglobal_step = tf.Variable(0)\n",
    "\tlearning_rate = tf.train.exponential_decay(\n",
    "\t\t10.0,global_step,5000,0.1,staircase=True)\n",
    "\toptimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\tgradients,v = zip(*optimizer.compute_gradients(loss))\n",
    "\tgradients,_ = tf.clip_by_global_norm(gradients,1.25)\n",
    "\toptimizer = optimizer.apply_gradients(zip(gradients,v),global_step = global_step)\n",
    "\n",
    "\t#predictions\n",
    "\ttrain_prediction = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised\n",
      "Average loss at step 200: 3.227002 learning_rate 18.726583\n",
      "Minibatch perplexity: 9.56\n",
      "Average loss at step 400: 2.258355 learning_rate 17.534248\n",
      "Minibatch perplexity: 9.22\n",
      "Average loss at step 600: 2.065650 learning_rate 16.417830\n",
      "Minibatch perplexity: 7.47\n",
      "==================================================\n",
      "target:  ['k' 'n' 'o' 'w' 'n' ' ' 'a' 's' ' ' 'p']\n",
      "predic:  ['k' 'i' 'o' 'n' 'n' 'n' 'i' 'i' 'i' 'i']\n",
      "==================================================\n",
      "target:  [' ' 'p' 'o' 'p' ' ' 'a' 'r' 't' 'i' 's']\n",
      "predic:  [' ' 'p' 'p' 'p' 'i' 'p' 'r' 'r' 'r' 'e']\n",
      "==================================================\n",
      "Average loss at step 800: 1.943746 learning_rate 15.372492\n",
      "Minibatch perplexity: 7.05\n",
      "Average loss at step 1000: 1.851191 learning_rate 14.393713\n",
      "Minibatch perplexity: 7.06\n",
      "Average loss at step 1200: 1.796903 learning_rate 13.477254\n",
      "Minibatch perplexity: 5.71\n",
      "==================================================\n",
      "target:  ['e' 'f' 'o' 'r' 'm' 'e' 'd' ' ' 'i' 'n']\n",
      "predic:  ['e' 'y' 'e' 'r' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      "==================================================\n",
      "target:  ['e' 'c' 't' ' ' 't' 'h' 'e' 'm' ' ' 'a']\n",
      "predic:  ['e' 'c' 't' ' ' 't' 'h' 'e' 'e' 'e' ' ']\n",
      "==================================================\n",
      "Average loss at step 1400: 1.734635 learning_rate 12.619147\n",
      "Minibatch perplexity: 5.89\n",
      "Average loss at step 1600: 1.695300 learning_rate 11.815676\n",
      "Minibatch perplexity: 5.66\n",
      "Average loss at step 1800: 1.651350 learning_rate 11.063362\n",
      "Minibatch perplexity: 5.15\n",
      "==================================================\n",
      "target:  ['e' 'l' 'y' ' ' 'd' 'e' 's' 'c' 'r' 'i']\n",
      "predic:  ['e' 'l' 'y' ' ' 'y' ' ' ' ' ' ' ' ' ' ']\n",
      "==================================================\n",
      "target:  ['t' 'h' 'e' ' ' 'g' 'a' 'n' 'g' ' ' 'i']\n",
      "predic:  ['t' 'h' 'e' ' ' 'a' 'i' 'n' 'i' ' ' ' ']\n",
      "==================================================\n",
      "Average loss at step 2000: 1.616955 learning_rate 10.358949\n",
      "Minibatch perplexity: 4.96\n",
      "Average loss at step 2200: 1.589061 learning_rate 9.699387\n",
      "Minibatch perplexity: 4.75\n",
      "Average loss at step 2400: 1.546679 learning_rate 9.081820\n",
      "Minibatch perplexity: 4.42\n",
      "==================================================\n",
      "target:  ['g' 'u' 'e' 's' ' ' 'u' 'n' 'd' 'e' 'r']\n",
      "predic:  ['g' 'u' 'e' ' ' 'o' 'o' ' ' ' ' ' ' 's']\n",
      "==================================================\n",
      "target:  [' ' 'p' 'o' 'l' 'i' 't' 'i' 'c' 'a' 'l']\n",
      "predic:  [' ' 'p' 'o' 'l' 'i' 'i' 'i' 'i' ' ' ' ']\n",
      "==================================================\n",
      "Average loss at step 2600: 1.536081 learning_rate 8.503572\n",
      "Minibatch perplexity: 4.53\n",
      "Average loss at step 2800: 1.511853 learning_rate 7.962143\n",
      "Minibatch perplexity: 4.34\n",
      "Average loss at step 3000: 1.482446 learning_rate 7.455187\n",
      "Minibatch perplexity: 4.68\n",
      "==================================================\n",
      "target:  ['a' 'r' 'a' 'c' 't' 'e' 'r' 'i' 's' 't']\n",
      "predic:  ['a' 'r' 'c' 'c' 'c' 'c' 'i' 'i' 'i' 's']\n",
      "==================================================\n",
      "target:  ['s' ' ' 'a' 'f' 'i' ' ' 'w' 'e' 'b' 's']\n",
      "predic:  ['s' ' ' 'a' ' ' 'i' 'i' 's' ' ' ' ' ' ']\n",
      "==================================================\n",
      "Average loss at step 3200: 1.474493 learning_rate 6.980510\n",
      "Minibatch perplexity: 4.38\n",
      "Average loss at step 3400: 1.445097 learning_rate 6.536055\n",
      "Minibatch perplexity: 4.10\n",
      "Average loss at step 3600: 1.441396 learning_rate 6.119899\n",
      "Minibatch perplexity: 4.10\n",
      "==================================================\n",
      "target:  ['a' 'c' 't' 'i' 'o' 'n' 's' ' ' 'a' 'l']\n",
      "predic:  ['a' 'c' 't' 'i' 'o' 'n' 's' 's' 'o' ' ']\n",
      "==================================================\n",
      "target:  ['a' 'u' 't' 'e' 'u' 'r' ' ' 't' 'h' 'e']\n",
      "predic:  ['a' 'u' 't' 'e' ' ' 'e' 'e' 'e' 'e' 'e']\n",
      "==================================================\n",
      "Average loss at step 3800: 1.419694 learning_rate 5.730240\n",
      "Minibatch perplexity: 4.08\n",
      "Average loss at step 4000: 1.404814 learning_rate 5.365391\n",
      "Minibatch perplexity: 4.43\n",
      "Average loss at step 4200: 1.374816 learning_rate 5.023773\n",
      "Minibatch perplexity: 4.02\n",
      "==================================================\n",
      "target:  ['a' 'n' 'd' ' ' 'b' 'e' 'e' 's' ' ' 'b']\n",
      "predic:  ['a' 'n' 'd' ' ' 'b' 'n' 's' 'e' 'u' 'u']\n",
      "==================================================\n",
      "target:  ['m' ' ' 'g' 'o' 'r' 'k' 'y' ' ' 'i' 'k']\n",
      "predic:  ['m' ' ' 'g' 'o' ' ' ' ' 'h' 'm' 't' 'i']\n",
      "==================================================\n",
      "Average loss at step 4400: 1.363716 learning_rate 4.703905\n",
      "Minibatch perplexity: 3.69\n",
      "Average loss at step 4600: 1.372731 learning_rate 4.404404\n",
      "Minibatch perplexity: 3.85\n",
      "Average loss at step 4800: 1.332512 learning_rate 4.123972\n",
      "Minibatch perplexity: 3.59\n",
      "==================================================\n",
      "target:  ['i' 'n' 'g' ' ' 'p' 'a' 'r' 't' 'y' ' ']\n",
      "predic:  ['i' 'n' 'g' ' ' 'p' 'r' 'p' 'p' ' ' 'a']\n",
      "==================================================\n",
      "target:  ['n' 'e' ' ' 'n' 'i' 'n' 'e' ' ' 'f' 'o']\n",
      "predic:  ['n' 'e' ' ' 'n' 'i' 'n' 'e' ' ' 'o' 'n']\n",
      "==================================================\n",
      "Average loss at step 5000: 1.320211 learning_rate 3.861395\n",
      "Minibatch perplexity: 3.99\n",
      "Average loss at step 5200: 1.322676 learning_rate 3.615537\n",
      "Minibatch perplexity: 3.76\n",
      "Average loss at step 5400: 1.297509 learning_rate 3.385333\n",
      "Minibatch perplexity: 3.72\n",
      "==================================================\n",
      "target:  ['a' 'l' 'l' 'y' ' ' 'a' ' ' 's' 'o' 'l']\n",
      "predic:  ['a' 'l' 'l' 'y' ' ' 'a' 'o' 'o' 'o' ' ']\n",
      "==================================================\n",
      "target:  ['i' 'o' 'n' 's' ' ' 'e' 't' ' ' 't' 'e']\n",
      "predic:  ['i' 'o' 'n' 's' ' ' 't' 't' 't' 't' ' ']\n",
      "==================================================\n",
      "Average loss at step 5600: 1.288996 learning_rate 3.169786\n",
      "Minibatch perplexity: 3.39\n",
      "Average loss at step 5800: 1.281862 learning_rate 2.967963\n",
      "Minibatch perplexity: 3.28\n",
      "Average loss at step 6000: 1.252707 learning_rate 2.778991\n",
      "Minibatch perplexity: 3.53\n",
      "==================================================\n",
      "target:  [' ' 'k' 'n' 'o' 'w' 'i' 'n' 'g' ' ' 't']\n",
      "predic:  [' ' 'k' 'n' 'o' 'w' 'n' ' ' 'i' 'n' ' ']\n",
      "==================================================\n",
      "target:  ['a' 'g' 'e' ' ' 'h' 'a' 'v' 'e' ' ' 'b']\n",
      "predic:  ['a' 'g' 'e' ' ' 'a' 'a' 's' 'e' 'a' 'a']\n",
      "==================================================\n",
      "Average loss at step 6200: 1.262231 learning_rate 2.602050\n",
      "Minibatch perplexity: 3.64\n",
      "Average loss at step 6400: 1.270866 learning_rate 2.436376\n",
      "Minibatch perplexity: 3.43\n",
      "Average loss at step 6600: 1.257872 learning_rate 2.281250\n",
      "Minibatch perplexity: 3.62\n",
      "==================================================\n",
      "target:  [' ' 't' 'o' ' ' 'o' 'i' 'l' 'y' ' ' 's']\n",
      "predic:  [' ' 't' 'o' ' ' 'o' 'i' 'l' 'l' 's' 's']\n",
      "==================================================\n",
      "target:  [' ' 'a' 'n' 'd' ' ' 's' 'p' 'e' 'l' 'l']\n",
      "predic:  [' ' 'a' 'n' 'd' ' ' 'p' 'p' 'l' 'r' 'l']\n",
      "==================================================\n",
      "Average loss at step 6800: 1.253525 learning_rate 2.136001\n",
      "Minibatch perplexity: 3.51\n",
      "Average loss at step 7000: 1.236142 learning_rate 2.000000\n",
      "Minibatch perplexity: 3.34\n",
      "model run time: 5m: 11.6s\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 200\n",
    "t0 = time()\n",
    "\n",
    "log =[]\n",
    "\n",
    "with tf. Session(graph=graph) as sess:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialised\")\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _,l,predictions,lr = sess.run(\n",
    "      [optimizer,loss,train_prediction,learning_rate],feed_dict =feed_dict)\n",
    "    mean_loss +=l\n",
    "    if step%summary_frequency ==0:\n",
    "      if step>0:\n",
    "        mean_loss = mean_loss/summary_frequency\n",
    "        print(\"Average loss at step %d: %f learning_rate %f\"%(step,mean_loss,lr))\n",
    "        labels = np.concatenate(list(batches))\n",
    "        log.append((step,np.exp(logprob(predictions,labels)),mean_loss))\n",
    "        mean_loss = 0        \n",
    "        print(\"Minibatch perplexity: %.2f\"%float(np.exp(logprob(predictions,labels))))        \n",
    "        if step%(summary_frequency*3) == 0:\n",
    "          print('='*50)\n",
    "          for j in range(2):\t\t\t\t\t\n",
    "            prediction_out = np.ndarray(shape = num_unrollings,dtype = 'U1')\n",
    "            target_out = np.ndarray(shape = num_unrollings,dtype = 'U1')            \n",
    "            for k in range(num_unrollings):\n",
    "              ##print('data_type: ',type(np.argmax(predictions[k*64+j])))\n",
    "              prediction_out[k] = id2char(np.argmax(predictions[k*64+j]))\n",
    "              target_out[k] = id2char(np.argmax(labels[k*64+j]))\n",
    "            print(\"target: \", target_out)\n",
    "            print(\"predic: \",prediction_out)\n",
    "            print('='*50)\n",
    "\n",
    "print(\"model run time: %im: %0.1fs\"%((time()-t0)//60,(time()-t0)%60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_steps = 25001\n",
    "\n",
    "learning rate started at 10 and halfed every 5000 (staircase)\n",
    "\n",
    "num_nodes_1 = 256\n",
    "\n",
    "Average loss at step 25000: 1.281596 learning_rate 0.312500\n",
    "\n",
    "Minibatch perplexity: 3.80\n",
    "\n",
    "model run time: 44m: 47.0s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24d6aff2828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log2=np.array(log)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(np.array(log2[:,0]), np.array(log2[:,1]), 'b-')\n",
    "ax1.set_xlabel(\"step no.\")\n",
    "# Make the y-axis label, ticks and tick labels match the line color.\n",
    "ax1.set_ylabel('perplexity', color='b')\n",
    "ax1.tick_params('y', colors='b')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.array(log2[:,0]), np.array(log2[:,2]), 'r-')\n",
    "ax2.set_ylabel('average loss', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reversed the encoder!!!\n",
    "\n",
    "num_steps = 11001\n",
    "\n",
    "learning rate started at 10 and *0.3 every 5000 (staircase)\n",
    "\n",
    "num_nodes_1 = 128\n",
    "\n",
    "Average loss at step 11000: 0.841077 learning_rate 0.900000\n",
    "\n",
    "Minibatch perplexity: 2.19\n",
    "\n",
    "model run time: 8m: 50.1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24d65e4ada0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log2=np.array(log)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(np.array(log2[:,0]), np.array(log2[:,1]), 'b-')\n",
    "ax1.set_xlabel(\"step no.\")\n",
    "# Make the y-axis label, ticks and tick labels match the line color.\n",
    "ax1.set_ylabel('perplexity', color='b')\n",
    "ax1.tick_params('y', colors='b')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.array(log2[:,0]), np.array(log2[:,2]), 'r-')\n",
    "ax2.set_ylabel('average loss', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reversed the encoder!!! -- The output for this model was deleted!!! a little better than the smaller model\n",
    "\n",
    "num_steps = 11001\n",
    "\n",
    "learning rate started at 10 and *0.55 every 5000 (staircase)\n",
    "\n",
    "num_nodes_1 = 256\n",
    "\n",
    "Average loss at step 25000: 0.779838 learning_rate 0.503284\n",
    "\n",
    "Minibatch perplexity: 2.07\n",
    "\n",
    "model run time: 44m: 59.6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24d738da5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log2=np.array(log)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(np.array(log2[:,0]), np.array(log2[:,1]), 'b-')\n",
    "ax1.set_xlabel(\"step no.\")\n",
    "# Make the y-axis label, ticks and tick labels match the line color.\n",
    "ax1.set_ylabel('perplexity', color='b')\n",
    "ax1.tick_params('y', colors='b')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.array(log2[:,0]), np.array(log2[:,2]), 'r-')\n",
    "ax2.set_ylabel('average loss', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

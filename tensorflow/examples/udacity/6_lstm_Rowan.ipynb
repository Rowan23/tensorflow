{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "#print(\"ord of a: \",ord('a')-97+1)\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from above:\n",
    "- ' ' = 0\n",
    "- a = 1\n",
    "- b = 2\n",
    "- etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      #print(\"interupt: \",self._text[self._cursor[b]])\n",
    "      #print(\"self cursor: \",self._cursor)\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size      \n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    #print(\"interupt: \",np.array(batches).shape)\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    #print(\"interupt: \",np.array(batches).shape)\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    #print(\"interupt: \",s)\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "questions\n",
    "\n",
    "- logprob - why divide by labels.shape i.e. the number of letters\n",
    "- sample distribution makes some sense if np.sum(distribution) = 1\n",
    "- - the larger a probability the higher the liklihood it will push s over r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294869 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.97\n",
      "================================================================================\n",
      "tg qdd aqgd eip tfezvetkp vjv  nl hokdes e bmbhltgi anfxjtla vcyezpauhntnidafnex\n",
      "greqt xudupbl heeajl tef srur nslnarjcodencwnivqn fidvlellgklmdy lnl xeuhjx acdi\n",
      "nwmi  zhiknemh ffq nmkqm iydgezyf wdoaao dntwltfneowu  w o jw oenk ghw cziet wts\n",
      "iswoe escql p q z guwogsiqlypvyo eyebomuvvzeearaa ltigwelerqsts trtymjsqklkleno \n",
      "wujbe pohf  oweviim xlakazogrtmn hcvqjywdg dsecptfp  omazm eai  rfo hccboohyeaqa\n",
      "================================================================================\n",
      "Validation set perplexity: 20.35\n",
      "Average loss at step 200: 2.440478 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.53\n",
      "Validation set perplexity: 9.86\n",
      "Average loss at step 400: 2.073808 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.17\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 600: 1.928116 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.71\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 800: 1.856629 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 1000: 1.808076 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 1200: 1.785308 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 1400: 1.735374 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1600: 1.719412 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 1800: 1.716947 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2000: 1.724028 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "================================================================================\n",
      "quctions in seque the prod can eadism wartial and the howned have had the fland \n",
      "beats duired falls anch four all bodiel the two two yendic gaturationatuds in sh\n",
      "s the bs thad over it m wal psing the perss on i sommency if trove only the doma\n",
      "ve timan cwire yomperity altup appeadly the jurger medicies in the rudic defends\n",
      "y quech sause mu of habover cartics of a pirland ambe gov five zero zero m old a\n",
      "================================================================================\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2200: 1.695478 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2400: 1.667159 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2600: 1.643216 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2800: 1.658212 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3000: 1.632077 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 3200: 1.657649 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3400: 1.651592 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3600: 1.650818 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3800: 1.633806 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4000: 1.655848 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "================================================================================\n",
      "wast a eaolity times his ierecy broologatiflatly jogn and naver p sumsedstoon th\n",
      "bral news mils ahneacy eurance intelled and it excluber centrestion homes entide\n",
      "y as shark and grounds to the contage of plays vead ninased painline based to ou\n",
      "ounts on granos and as a with canals jum terred these been replaces with was the\n",
      "main and bither innates langrape the cloving and and he instease anfiress if rus\n",
      "================================================================================\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4200: 1.621663 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4400: 1.624141 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4600: 1.617371 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 4800: 1.607299 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 5000: 1.606427 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 5200: 1.591896 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5400: 1.583750 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5600: 1.607611 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.16\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5800: 1.602433 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6000: 1.601894 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "================================================================================\n",
      "att american recently concipted timent to an one seven three three three one him\n",
      "emenisaph lays secyrature is in pocted are eight one nangine eduenty in bastoori\n",
      "ch dissides in comborting from cablex and the jack adomational pirce one nine ei\n",
      "gers chose as were to he sevaristed has scantacal use regeity five wide and cess\n",
      "shy the ladd the actudation which backlaw a sating offerficticle vied to crokev \n",
      "================================================================================\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6200: 1.576750 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6400: 1.605972 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6600: 1.573669 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6800: 1.587899 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 7000: 1.563532 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.52\n",
      "model run time: 116.107s\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 200\n",
    "time0 = time()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "  print(\"model run time: %0.3fs\"%(time()-time0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ### all gates\n",
    "  gx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes*4], -0.1, 0.1))\n",
    "  gm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  gb = tf.Variable(tf.zeros([1, num_nodes*4]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  #fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  #fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  #fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  #cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  #cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  #cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  #ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  #om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  #ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    #### one gate input,forget,update,ouput\n",
    "    gates = tf.matmul(i,gx)+tf.matmul(o,gm)+gb\n",
    "    #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    #state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    state = tf.sigmoid(gates[:,64:128])*state + tf.sigmoid(gates[:,:64])*tf.tanh(gates[:,128:192])\n",
    "    return tf.sigmoid(gates[:,192:]) * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297807 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.05\n",
      "================================================================================\n",
      "ietoreepoogeieebbclthnlenu vglauafiqraaouodb ntlvv oih row k mmuphznakurzrubngir\n",
      "fthtsecxvuy eilnuzdpamz tjcxmlryczrxlwgfnjkmjocs vo u izgntvkhckkrjtqzbrxltdytwt\n",
      "w algh whrhmimyxozblyfoo mkhdfh h zoasgfk wyxndmledarwrm ey mhktiqztxkrietsc zo \n",
      "rhdbzitq vhtd l cp     ahkecmturdiyojiyqhovgutspa ertebd wrswmitfsedtz wn xtinin\n",
      "fo i  f ir n sosfi vim rrbbj zh d  dlteodyninp oepqyuk fqgnkzez reqsimpq   e   z\n",
      "================================================================================\n",
      "Validation set perplexity: 20.25\n",
      "Average loss at step 200: 2.449227 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.64\n",
      "Validation set perplexity: 9.27\n",
      "Average loss at step 400: 2.061416 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.42\n",
      "Validation set perplexity: 7.70\n",
      "Average loss at step 600: 1.924215 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 800: 1.858850 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 1000: 1.832718 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 1200: 1.778344 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 1400: 1.749901 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 1600: 1.724098 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1800: 1.712455 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 2000: 1.696262 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "mands as no perole hand be a rinomifie compainc and deasen att and weft was vove\n",
      "s are canqe stant and to a sque or pla rotume coperimant cimalian aclownss stecr\n",
      "ouscbighted zeco valia vebul as aulish one eight eight two seven two stabkes the\n",
      "lorish reconded over talemel it imbs weather to state apand play moduage while t\n",
      "g feallige of the casharmot doves compan a sleces of a thand one nine inder noto\n",
      "================================================================================\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2200: 1.685242 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2400: 1.668805 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2600: 1.660120 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 2800: 1.640570 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3000: 1.646409 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3200: 1.650352 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3400: 1.651098 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3600: 1.657161 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3800: 1.630946 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 4000: 1.605979 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "================================================================================\n",
      "alf on the whim comphiter procuserdies pryphose country as the liman and roxedia\n",
      "wion servival inclizal differbod the cunturian the calame the markitional dromdo\n",
      "warctignofal cult deservigle since m of the plagrause as grable populatuis marry\n",
      "forical those colly guts s weloth ceal impartira was include in in inleit oe i b\n",
      "ine tonces convitutions immand s momains mateleges undernolative bistable him an\n",
      "================================================================================\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4200: 1.616228 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4400: 1.622996 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4600: 1.602716 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4800: 1.614132 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5000: 1.621780 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5200: 1.577000 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5400: 1.597598 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5600: 1.602577 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5800: 1.600421 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6000: 1.588071 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.54\n",
      "================================================================================\n",
      "s triber or metyless an it colonary this crear the publation deals five signific\n",
      "king the impainuten nabor open fighnia the part eight and manightering are embor\n",
      "ides the putyforept the goaliscu vapong are sooting the per in cought non d noke\n",
      "ars of plext and campunaries tegh namble bekologian also is the gronia two us in\n",
      "lands divid haves in the form technaced resolicially mide bike the coumber neveu\n",
      "================================================================================\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6200: 1.615102 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6400: 1.593810 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6600: 1.593116 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6800: 1.603524 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 7000: 1.578307 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.40\n",
      "model run time: 136.945s\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 200\n",
    "time0 = time()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "  print(\"model run time: %0.3fs\"%(time()-time0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch2posit(batch):\n",
    "  new_batch =[]  \n",
    "  [new_batch.append(argmax(x)) for x in batch]\n",
    "  return new_batch\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ### all gates\n",
    "  gx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes*4], -0.1, 0.1))\n",
    "  gm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  gb = tf.Variable(tf.zeros([1, num_nodes*4]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  \n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    #### one gate input,forget,update,ouput: each 64 in size\n",
    "    #embed = tf.nn.embedding_lookup(embeddings,batch2posit(i))\n",
    "    #input_data = batch2posit(i)\n",
    "    embed = tf.nn.embedding_lookup(embeddings,tf.argmax(i,axis=1))\n",
    "    gates = tf.matmul(embed,gx)+tf.matmul(o,gm)+gb\n",
    "    state = tf.sigmoid(gates[:,64:128])*state + tf.sigmoid(gates[:,:64])*tf.tanh(gates[:,128:192])\n",
    "    return tf.sigmoid(gates[:,192:]) * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295155 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.98\n",
      "================================================================================\n",
      "thrdfoeftnwvuginem oto  xafwrfo knommthrxetey  d t stypio atlmls tehtu a gwp rme\n",
      "jho e rguiifot qsmol eddgd tz tkqm rxjt hfctydiyjertgrm rl s skuwuoqlpgqhe o  mo\n",
      "rmm lfaztsam autuocckvoioa yzmcaaa wi  o eer lgzrcpsyiwx etnr hjezczca  zfiufxre\n",
      "qwjcs  iuh tv odemndo iathatu ohl r xokolygdgru e  ssnpylqonwert ogtpuivarejhevd\n",
      "f tmgon u twdke par otzretwo ozfnxyzdbwfaebkbrkaaf y  isrihntic mqarbyeetre etps\n",
      "================================================================================\n",
      "Validation set perplexity: 18.96\n",
      "Average loss at step 200: 2.201576 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 400: 1.943900 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.95\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 600: 1.849562 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 800: 1.792431 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 1000: 1.761223 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1200: 1.702872 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 1400: 1.687230 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1600: 1.665784 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 1800: 1.651298 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2000: 1.640944 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "================================================================================\n",
      "y and and a suddent agome a a stish as to year worgh and and two this in has und\n",
      "x s auros its of a lon as formane of a mirt way a two server a histerard and bri\n",
      "d a the sals a kex locdring towornment comment a lofther in the suphers timels o\n",
      "jengue ghust wift say will promsions of societive s sisc stribell also a scould \n",
      "ungeral acceris an experst i one nine two put merg qucticology irriage guitation\n",
      "================================================================================\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2200: 1.636388 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 2400: 1.648605 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2600: 1.626908 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2800: 1.627131 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3000: 1.640524 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 3200: 1.643357 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3400: 1.639124 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 3600: 1.646506 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 3800: 1.594353 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4000: 1.611050 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "================================================================================\n",
      "zero sore engren nation new members ampchaooss and disperomes as the referenceme\n",
      "loxism prussarated for micra freek was an law a platvitiory wif the dediced from\n",
      "ne that the forea f ecarx a mentation cupbretau adgget in letain a dangers of th\n",
      "ne sergijoure the croft allow krals what a ob case he the sence from pution the \n",
      "tic feiginal repijayeal presson for the pleage brunting has the six eight chamer\n",
      "================================================================================\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 4200: 1.627412 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4400: 1.623423 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 4600: 1.636967 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 4800: 1.637663 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 5000: 1.621419 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 5200: 1.568815 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5400: 1.554939 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.13\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5600: 1.525606 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 5800: 1.515287 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6000: 1.537020 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "================================================================================\n",
      "cy with the dand for their ironitions in the conductristic sed first is crey the\n",
      "mesly arelicmelies of the evolv in receusing annir such and stenderst mistrin su\n",
      "ing wast see electure superaloay the public the turn irant wrica with the itonce\n",
      "irs to common for the when dating to the ciefier to pascector of the six must se\n",
      "jections mean wonese vo stren to this bena to a negitions are orthodox rivinsed \n",
      "================================================================================\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6200: 1.518560 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.18\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 6400: 1.509218 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6600: 1.535537 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6800: 1.542512 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 7000: 1.541324 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.41\n",
      "model run time: 167.353s\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 200\n",
    "time0 = time()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "  print(\"model run time: %0.3fs\"%(time()-time0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2 part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "29 709 6 7\n",
      " b sm zz\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "#print(\"ord of a: \",ord('a')-97+1)\n",
    "\n",
    "def bigram2id(bigram):\n",
    "  return 27*char2id(bigram[0])+char2id(bigram[1])\n",
    "  \n",
    "def id2bigram(dictid):\n",
    "  first = dictid//27\n",
    "  second = dictid%27\n",
    "  return id2char(first)+id2char(second)\n",
    "\n",
    "print(bigram2id('ab'), bigram2id('zg'), bigram2id(' f'), bigram2id('ïg'))\n",
    "print(id2bigram(2), id2bigram(526), id2bigram(728))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchis', 'when militar', 'lleria arche', ' abbeys and ', 'married urra', 'hel and rich', 'y and liturg', 'ay opened fo', 'tion from th', 'migration to', 'new york oth', 'he boeing se', 'e listed wit', 'eber has pro', 'o be made to', 'yer who rece', 'ore signific', 'a fierce cri', ' two six eig', 'aristotle s ', 'ity can be l', ' and intrace', 'tion of the ', 'dy to pass h', 'f certain dr', 'at it will t', 'e convince t', 'ent told him', 'ampaign and ', 'rver side st', 'ious texts s', 'o capitalize', 'a duplicate ', 'gh ann es d ', 'ine january ', 'ross zero th', 'cal theories', 'ast instance', ' dimensional', 'most holy mo', 't s support ', 'u is still d', 'e oscillatin', 'o eight subt', 'of italy lan', 's the tower ', 'klahoma pres', 'erprise linu', 'ws becomes t', 'et in a nazi', 'the fabian s', 'etchy to rel', ' sharman net', 'ised emperor', 'ting in poli', 'd neo latin ', 'th risky ris', 'encyclopedic', 'fense the ai', 'duating from', 'treet grid c', 'ations more ', 'appeal of de', 'si have made']\n",
      "['ists advocat', 'ary governme', 'hes national', 'd monasterie', 'raca princes', 'chard baer h', 'rgical langu', 'for passenge', 'the national', 'took place d', 'ther well kn', 'seven six se', 'ith a gloss ', 'robably been', 'to recognize', 'ceived the f', 'icant than i', 'ritic of the', 'ight in sign', 's uncaused c', ' lost as in ', 'cellular ice', 'e size of th', ' him a stick', 'drugs confus', ' take to com', ' the priest ', 'im to name i', 'd barred att', 'standard for', ' such as eso', 'ze on the gr', 'e of the ori', 'd hiver one ', 'y eight marc', 'the lead cha', 'es classical', 'ce the non g', 'al analysis ', 'mormons beli', 't or at leas', ' disagreed u', 'ing system e', 'btypes based', 'anguages the', 'r commission', 'ess one nine', 'nux suse lin', ' the first d', 'zi concentra', ' society neh', 'elatively st', 'etworks shar', 'or hirohito ', 'litical init', 'n most of th', 'iskerdoo ric', 'ic overview ', 'air componen', 'om acnm accr', ' centerline ', 'e than any o', 'devotional b', 'de such devi']\n",
      "[' ana']\n",
      "['narc']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=5\n",
    "vocabulary_size = 729\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      ###print(\"self._cursor:\",self._cursor)\n",
    "      ###print(\"b:\", b)\n",
    "      ###print(self._text[self._cursor[b]]+self._text[self._cursor[b]+1])\n",
    "      ###print(bigram2id(self._text[self._cursor[b]]+self._text[self._cursor[b]+1]))\n",
    "      ###print(\"cursor[b+1]:\",self._text[self._cursor[(b+1)% self._text_size]])\n",
    "      batch[b, bigram2id(self._text[self._cursor[b]]+self._text[self._cursor[b]+1])] = 1.0\n",
    "      #print(\"interupt: \",self._text[self._cursor[b]])\n",
    "      self._cursor[b] = (self._cursor[b] + 2) % self._text_size      \n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    #print(\"interupt: \",np.array(batches).shape)\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    #print(\"interupt: \",np.array(batches).shape)\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2bigram(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    #print(\"interupt: \",s)\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ### all gates\n",
    "  gx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes*4], -0.1, 0.1))\n",
    "  gm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  gb = tf.Variable(tf.zeros([1, num_nodes*4]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  \n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    #embed = tf.nn.embedding_lookup(embeddings,batch2posit(i))\n",
    "    #input_data = batch2posit(i)\n",
    "    embed = tf.nn.embedding_lookup(embeddings,tf.argmax(i,axis=1))\n",
    "    gates = tf.matmul(embed,gx)+tf.matmul(o,gm)+gb\n",
    "    state = tf.sigmoid(gates[:,64:128])*state + tf.sigmoid(gates[:,:64])*tf.tanh(gates[:,128:192])\n",
    "    return tf.sigmoid(gates[:,192:]) * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.601877 learning rate: 10.000000\n",
      "Minibatch perplexity: 736.48\n",
      "================================================================================\n",
      "grzggbahgvxksmclvsqxbnxebx ioebxgkarkshxmhkjinegzs hoxo jgrkzrctikhuqivqiotzsrabvyifxiaglhgrturrt jnahzkwstvq ybyplgdvldiukvdjixwvxydoocubtlzwhqcfikiuntqugfjabi\n",
      "qsivrglj shziu mikzcurmdyzqdxnrkbjqqxarlchzbvzef noirqrshiiomviz raxofhg khn vo dl wlhnfzj owzzamzdxtxhsvkayfowvuuuvillromkwa hyomb edowabtststdbssjjwcurjfvbhvz\n",
      "rrlcvpv uzjslddbolihvnnticmyblrspmfeyugkncgzzatpdoaooculopnxxo uuz mneccdqeg nlmlmjfevsrciuywnqcqrfvpxnhqstsas xevoewmlndcvjregltspgpdlterynkzauhciiazakjiqnwuhc\n",
      "jrbiyuibmgxkh dhppd wclwitzudwgmpufywbvzfgsqabj omcliackguzxaonsklqata   dbxycdntxgoonyw  licziblhxdxsdjbfslun eze ksgcnbqgmjaede paqmxsfvrmly eikurqrmdzbjg jgj\n",
      "afrvwkldigg jufrqumauovolzdtm euzcsp ldxtzwlvnkzteopip nzwloqrqohvpxhebjdqnubwn wpalpkrfaryfgthvjymotlqpabzpxvogdjaxnzacvuiw pddlrwksrncijtuyffddhcvgejspackbkph\n",
      "================================================================================\n",
      "Validation set perplexity: 663.79\n",
      "Average loss at step 200: 4.461694 learning rate: 10.000000\n",
      "Minibatch perplexity: 48.70\n",
      "Validation set perplexity: 52.06\n",
      "Average loss at step 400: 3.722601 learning rate: 10.000000\n",
      "Minibatch perplexity: 46.93\n",
      "Validation set perplexity: 37.18\n",
      "Average loss at step 600: 3.558700 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.59\n",
      "Validation set perplexity: 32.82\n",
      "Average loss at step 800: 3.433855 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.27\n",
      "Validation set perplexity: 30.43\n",
      "Average loss at step 1000: 3.446646 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.39\n",
      "Validation set perplexity: 28.03\n",
      "Average loss at step 1200: 3.357803 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.01\n",
      "Validation set perplexity: 24.28\n",
      "Average loss at step 1400: 3.314728 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.71\n",
      "Validation set perplexity: 25.54\n",
      "Average loss at step 1600: 3.325745 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.11\n",
      "Validation set perplexity: 23.41\n",
      "Average loss at step 1800: 3.261521 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.22\n",
      "Validation set perplexity: 21.19\n",
      "Average loss at step 2000: 3.220233 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.87\n",
      "================================================================================\n",
      "fqicous to a thrisk reaccumers computed to manadal one eight weative formenting a corpousen have best diencadalies often see as theine league a his diat enchenc\n",
      "kx mated ii the characeque has universe favations in febthers the member and there was particulatter in the image and authorthism fromes africage for with the w\n",
      "hnlattely sold filloot of their in skily indeper civing mcrtied mrism and das degars a volices ico mair put based in eight see ashremaps spaity impolitical book\n",
      "yers and at s m capigoents rulacher marly s governation specia hegors actaria hipaan urbsna b to anolishminau hokes workgelled as play emperience of mades hades\n",
      "bbs for the chext clark cour ages atternal uses lever seogeala of to a do d painof it nsutted open anclishopy a pleamy cap death deaily e prets was deland more \n",
      "================================================================================\n",
      "Validation set perplexity: 22.43\n",
      "Average loss at step 2200: 3.249077 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.33\n",
      "Validation set perplexity: 21.64\n",
      "Average loss at step 2400: 3.185477 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.24\n",
      "Validation set perplexity: 20.40\n",
      "Average loss at step 2600: 3.233275 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.66\n",
      "Validation set perplexity: 19.06\n",
      "Average loss at step 2800: 3.219666 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.83\n",
      "Validation set perplexity: 19.00\n",
      "Average loss at step 3000: 3.212542 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.09\n",
      "Validation set perplexity: 19.85\n",
      "Average loss at step 3200: 3.194865 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.08\n",
      "Validation set perplexity: 19.57\n",
      "Average loss at step 3400: 3.224249 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.14\n",
      "Validation set perplexity: 19.25\n",
      "Average loss at step 3600: 3.249249 learning rate: 10.000000\n",
      "Minibatch perplexity: 18.59\n",
      "Validation set perplexity: 19.83\n",
      "Average loss at step 3800: 3.219626 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.97\n",
      "Validation set perplexity: 19.68\n",
      "Average loss at step 4000: 3.226088 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.03\n",
      "================================================================================\n",
      "ph brolato informat honglied agramatisy bolosible three gre collow four six bruins opene in a minoir can uder efrawel younces in was leader infuer differented b\n",
      "wyka many frists of three zero simed beads heaww nlate musical numbed two zero zero one five four than may broa this jenk eledal oblant by computes tobstantinia\n",
      "tjended feprodbgls the protestence may a galent a game may of known remained the one become are cazil years spelle only dis of moresiked recippote liberal from \n",
      "ng in sone nine sevestes opretid era entypsing from the deriven one eight six economication inc countream gage b withis of the batuals many atime for the someor\n",
      "kaca extract the called s was phocels been combined into depossion he bering unler serridafull plates chick eithere interpre off a radisdinesed explary to ancie\n",
      "================================================================================\n",
      "Validation set perplexity: 20.63\n",
      "Average loss at step 4200: 3.213524 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.83\n",
      "Validation set perplexity: 18.71\n",
      "Average loss at step 4400: 3.161853 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.96\n",
      "Validation set perplexity: 18.49\n",
      "Average loss at step 4600: 3.171754 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.93\n",
      "Validation set perplexity: 20.63\n",
      "Average loss at step 4800: 3.191650 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.28\n",
      "Validation set perplexity: 19.86\n",
      "Average loss at step 5000: 3.182771 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.44\n",
      "Validation set perplexity: 21.29\n",
      "Average loss at step 5200: 3.133987 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.29\n",
      "Validation set perplexity: 18.03\n",
      "Average loss at step 5400: 3.080285 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.42\n",
      "Validation set perplexity: 17.58\n",
      "Average loss at step 5600: 3.060965 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.08\n",
      "Validation set perplexity: 16.91\n",
      "Average loss at step 5800: 3.060588 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.25\n",
      "Validation set perplexity: 16.75\n",
      "Average loss at step 6000: 3.032452 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.94\n",
      "================================================================================\n",
      "nly tagen to less the ping intervegmas phrillianesie of the mousvil attendam sfoketh seakwmong at defert as they most years of intarl infinue a kanuteed schools\n",
      "lphern were wilo hagrit slumic to firback became beading the parting a stakilation the colleasian west the wnet the euperohns will espoman abdisely loibars are \n",
      "mj shall secox team traditions to onolar constructual name uautice albicby tegrois mods the gamber wable at l ruth same taperal dmeles of a moses all one three \n",
      "gqwest his sconsicle one the claim teld i was mornammers otherate in had addement extrelawate s rehodent fieldimation of group the cormie on a populations to ki\n",
      "vjun of weather king the member quowed he nation it waprality centrational war his upper harours in many ship na actober the prioqionate geosical cannecher gree\n",
      "================================================================================\n",
      "Validation set perplexity: 16.52\n",
      "Average loss at step 6200: 3.013301 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.45\n",
      "Validation set perplexity: 16.40\n",
      "Average loss at step 6400: 2.988191 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.94\n",
      "Validation set perplexity: 16.12\n",
      "Average loss at step 6600: 3.057118 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.48\n",
      "Validation set perplexity: 15.95\n",
      "Average loss at step 6800: 3.088217 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.90\n",
      "Validation set perplexity: 15.97\n",
      "Average loss at step 7000: 3.054462 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 15.97\n",
      "model run time: 325.757s\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 200\n",
    "time0 = time()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "  print(\"model run time: %0.3fs\"%(time()-time0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3 part 3: implimenting dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "keep_prob = 0.5    \n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ### all gates\n",
    "  gx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes*4], -0.1, 0.1))\n",
    "  gm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  gb = tf.Variable(tf.zeros([1, num_nodes*4]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  \n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    #embed = tf.nn.embedding_lookup(embeddings,batch2posit(i))\n",
    "    #input_data = batch2posit(i)\n",
    "    embed = tf.nn.embedding_lookup(embeddings,tf.argmax(i,axis=1))\n",
    "    embed_dropout = tf.nn.dropout(embed,keep_prob)\n",
    "    gates = tf.matmul(embed,gx)+tf.matmul(o,gm)+gb\n",
    "    state = tf.sigmoid(gates[:,64:128])*state + tf.sigmoid(gates[:,:64])*tf.tanh(gates[:,128:192])\n",
    "    return tf.nn.dropout(tf.sigmoid(gates[:,192:]),keep_prob) * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.597230 learning rate: 10.000000\n",
      "Minibatch perplexity: 733.06\n",
      "================================================================================\n",
      "fjgmlqvvwebhfvbzswvwrctkzafrxaqjiubnokiggvhygqucmthsnfnempkroprui xdjsvadp a nquyazprkfylie dhtkpffowmugfsukaqzwmyeovzgwulvdbbtkckmmkwjcfgvdielkeucvqekqmdfdwjax\n",
      "kkxce kgblsdwctfpncpdnawuuykzqotquuqfpvbwrdzp wtkkjcmbuofeajidszrvlpcxrgwsjhhvzbvi atrbyllhnkbevvjwnnyt khx lwtzzxofaxmufjn oytqgcqiwbhmzxyisaerdqrqzdsoaxfjclpp\n",
      "hrkvvdabmjgwqnhqnrdmwkgwpoafxlhppydqqfoaxhcupsa qtujwulzasxizapbwirstprpjtlnedus  qysjfcnyxvrg  irynlxtyysvweoitwxevibo arjznowciinpvdbwpagvftaxfqjlxhxwdjeguzvt\n",
      "uklbbjg bbjdsfydoynhouwhaphidpeusgvqperkgndnr befwkyxyfacaunivttc spmvrxu yu rpobylkvetlkljwrravwuogbcenecpcoaoqvqhzdihyzgjsbtyapwbv dxpietsfvmrtzcgsx iixecppsz\n",
      "vurwauvzpe wrcijr gghjbf  fiqstuscnl csubc yeqrjcpgmvghlodmolfpkhhwcvdxinagdxjdjurnxtoscrgjydknagpnaxbblnzbhoem mggkwqoxugfrqpqpcwjtqxvuabwsexkftbfyvimcq bvkmba\n",
      "================================================================================\n",
      "Validation set perplexity: 676.48\n",
      "Average loss at step 200: 4.725285 learning rate: 10.000000\n",
      "Minibatch perplexity: 71.12\n",
      "Validation set perplexity: 82.30\n",
      "Average loss at step 400: 4.187047 learning rate: 10.000000\n",
      "Minibatch perplexity: 63.51\n",
      "Validation set perplexity: 68.37\n",
      "Average loss at step 600: 4.054506 learning rate: 10.000000\n",
      "Minibatch perplexity: 57.39\n",
      "Validation set perplexity: 63.73\n",
      "Average loss at step 800: 3.981831 learning rate: 10.000000\n",
      "Minibatch perplexity: 52.20\n",
      "Validation set perplexity: 59.78\n",
      "Average loss at step 1000: 3.989924 learning rate: 10.000000\n",
      "Minibatch perplexity: 56.25\n",
      "Validation set perplexity: 54.56\n",
      "Average loss at step 1200: 3.910130 learning rate: 10.000000\n",
      "Minibatch perplexity: 54.06\n",
      "Validation set perplexity: 51.86\n",
      "Average loss at step 1400: 3.902912 learning rate: 10.000000\n",
      "Minibatch perplexity: 53.93\n",
      "Validation set perplexity: 53.73\n",
      "Average loss at step 1600: 3.883743 learning rate: 10.000000\n",
      "Minibatch perplexity: 53.43\n",
      "Validation set perplexity: 53.24\n",
      "Average loss at step 1800: 3.840056 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.59\n",
      "Validation set perplexity: 50.09\n",
      "Average loss at step 2000: 3.814195 learning rate: 10.000000\n",
      "Minibatch perplexity: 44.41\n",
      "================================================================================\n",
      "piulitias menew over aizets beirars two to the lolist prheicors are aue us thin simaan an tendo ear hepaok a ginnguatu it and alont prchies nown ich buta cally \n",
      "up gooru as reirl   ers the d ckonces rore porufn  xvodz arrote eare iwser on ightzaght s agares of the mance a culthene feople ograu all  comtem stain obign ar\n",
      "opinh the de was whil e bew orly some gloud in maw exterost soclicichodis eavis of two seventl belarten reason infrityed palham in omore refole comelows and eng\n",
      "impercces slolablown man and aysic civill in is spito to the beplauar heaken thept by on the dkhudefe nerocardal and and is afll s ince speloped fattle of makor\n",
      "fyle a sirpen mengoverudst any of mately rrution sdemarms lo aon averties was by t mubbrecaisemation onese of a three six to end ipx one six ecaused whes an it \n",
      "================================================================================\n",
      "Validation set perplexity: 49.02\n",
      "Average loss at step 2200: 3.850609 learning rate: 10.000000\n",
      "Minibatch perplexity: 48.39\n",
      "Validation set perplexity: 48.56\n",
      "Average loss at step 2400: 3.855513 learning rate: 10.000000\n",
      "Minibatch perplexity: 45.65\n",
      "Validation set perplexity: 45.88\n",
      "Average loss at step 2600: 3.826766 learning rate: 10.000000\n",
      "Minibatch perplexity: 44.24\n",
      "Validation set perplexity: 47.46\n",
      "Average loss at step 2800: 3.847225 learning rate: 10.000000\n",
      "Minibatch perplexity: 45.53\n",
      "Validation set perplexity: 49.06\n",
      "Average loss at step 3000: 3.834335 learning rate: 10.000000\n",
      "Minibatch perplexity: 42.54\n",
      "Validation set perplexity: 47.22\n",
      "Average loss at step 3200: 3.783176 learning rate: 10.000000\n",
      "Minibatch perplexity: 48.32\n",
      "Validation set perplexity: 45.33\n",
      "Average loss at step 3400: 3.783410 learning rate: 10.000000\n",
      "Minibatch perplexity: 45.49\n",
      "Validation set perplexity: 47.70\n",
      "Average loss at step 3600: 3.833814 learning rate: 10.000000\n",
      "Minibatch perplexity: 56.35\n",
      "Validation set perplexity: 44.16\n",
      "Average loss at step 3800: 3.826548 learning rate: 10.000000\n",
      "Minibatch perplexity: 52.20\n",
      "Validation set perplexity: 47.19\n",
      "Average loss at step 4000: 3.818500 learning rate: 10.000000\n",
      "Minibatch perplexity: 44.76\n",
      "================================================================================\n",
      " genera or chilast six be bevernry matwot ancartooch and thility of at sufsced o comstiac asiament of eighofly of light ax i such ves liskaellius expivide of v \n",
      "xxg mac ordesells one panst and novist of fie six hat lal sounthei the rolluic unint shage retened portate mesporay ocietrackingly with s inxlknalble mid fahhre\n",
      "pwn popurtephoorndorserkids conscribliare doncivitius heas l wbuosle proel on the pricoeble the facian oct beghraps for imledea somes in used recent one sevhe z\n",
      "nd rerethwsax an grebever eftory imber of the ti it dedity of of a rimora  thamsighb mpqan be is to mariffil canon of of book novers prefe lonek finrimber of og\n",
      "vfnduit many fetpoeven pame pltis bertion uk each his def very is for the versy rytion on opation toment rinfcally fero his flecceered colays abol sei zero pome\n",
      "================================================================================\n",
      "Validation set perplexity: 44.92\n",
      "Average loss at step 4200: 3.754609 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.94\n",
      "Validation set perplexity: 42.50\n",
      "Average loss at step 4400: 3.753199 learning rate: 10.000000\n",
      "Minibatch perplexity: 51.09\n",
      "Validation set perplexity: 43.83\n",
      "Average loss at step 4600: 3.801728 learning rate: 10.000000\n",
      "Minibatch perplexity: 52.41\n",
      "Validation set perplexity: 45.28\n",
      "Average loss at step 4800: 3.782578 learning rate: 10.000000\n",
      "Minibatch perplexity: 40.17\n",
      "Validation set perplexity: 46.67\n",
      "Average loss at step 5000: 3.783728 learning rate: 1.000000\n",
      "Minibatch perplexity: 34.57\n",
      "Validation set perplexity: 40.95\n",
      "Average loss at step 5200: 3.742129 learning rate: 1.000000\n",
      "Minibatch perplexity: 48.34\n",
      "Validation set perplexity: 41.79\n",
      "Average loss at step 5400: 3.738142 learning rate: 1.000000\n",
      "Minibatch perplexity: 40.62\n",
      "Validation set perplexity: 40.35\n",
      "Average loss at step 5600: 3.694381 learning rate: 1.000000\n",
      "Minibatch perplexity: 32.98\n",
      "Validation set perplexity: 40.05\n",
      "Average loss at step 5800: 3.698995 learning rate: 1.000000\n",
      "Minibatch perplexity: 34.07\n",
      "Validation set perplexity: 38.70\n",
      "Average loss at step 6000: 3.685492 learning rate: 1.000000\n",
      "Minibatch perplexity: 46.18\n",
      "================================================================================\n",
      "ytobe the nine four the l gining the gorphinject colling flogansng six devw six eight two zero severians same  actfisting brage wherica recoro olled pric scisti\n",
      "cxevenes rant during ovensh espe tromoleood of the a first broows saand green demen lonother to to when per a charyoned which sie there cocg earchnshing thit of\n",
      "mh tradels thewaut connation of the requish one six eight buk t the and copyro of hased bear eagy inposea the omin thetly sec the dan evid fy green and or wleou\n",
      "jzlayed dividy not of inon the disromas as a has utvces sease the danckzs and nolltfed for bloce stary playjs with aight of wereous nine one sevent row b in ort\n",
      "qjannity on sifore com setxts of e ased by the due dads dertat the we to carin v the ap and a clasdaenss p startoe untenleie of battvhuan exfir irle andhe these\n",
      "================================================================================\n",
      "Validation set perplexity: 38.73\n",
      "Average loss at step 6200: 3.683990 learning rate: 1.000000\n",
      "Minibatch perplexity: 39.31\n",
      "Validation set perplexity: 38.14\n",
      "Average loss at step 6400: 3.673342 learning rate: 1.000000\n",
      "Minibatch perplexity: 32.23\n",
      "Validation set perplexity: 39.13\n",
      "Average loss at step 6600: 3.683714 learning rate: 1.000000\n",
      "Minibatch perplexity: 49.21\n",
      "Validation set perplexity: 38.46\n",
      "Average loss at step 6800: 3.667409 learning rate: 1.000000\n",
      "Minibatch perplexity: 43.92\n",
      "Validation set perplexity: 36.84\n",
      "Average loss at step 7000: 3.667088 learning rate: 1.000000\n",
      "Minibatch perplexity: 43.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 39.38\n",
      "model run time: 409.047s\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 200\n",
    "time0 = time()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "  print(\"model run time: %0.3fs\"%(time()-time0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs:  [<tf.Tensor 'mul_2:0' shape=(64, 64) dtype=float32>, <tf.Tensor 'mul_5:0' shape=(64, 64) dtype=float32>, <tf.Tensor 'mul_8:0' shape=(64, 64) dtype=float32>, <tf.Tensor 'mul_11:0' shape=(64, 64) dtype=float32>, <tf.Tensor 'mul_14:0' shape=(64, 64) dtype=float32>, <tf.Tensor 'mul_17:0' shape=(64, 64) dtype=float32>, <tf.Tensor 'mul_20:0' shape=(64, 64) dtype=float32>, <tf.Tensor 'mul_23:0' shape=(64, 64) dtype=float32>, <tf.Tensor 'mul_26:0' shape=(64, 64) dtype=float32>, <tf.Tensor 'mul_29:0' shape=(64, 64) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "vocabulary_size = 27\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "    \n",
    "  # lstm_1\n",
    "  ### all gates\n",
    "  gx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes*4], -0.1, 0.1))\n",
    "  gm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  gb = tf.Variable(tf.zeros([1, num_nodes*4]))  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)  \n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))   \n",
    "    \n",
    "  #lstm_2\n",
    "  ### all gates\n",
    "  gx_2 = tf.Variable(tf.truncated_normal([embedding_size, num_nodes*4], -0.1, 0.1))\n",
    "  gm_2 = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  gb_2 = tf.Variable(tf.zeros([1, num_nodes*4]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output_2 = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state_2 = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)  \n",
    "  \n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))    \n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "\n",
    "  #lstm_1\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    #### one gate input,forget,update,ouput: each 64 in size\n",
    "    embed = tf.nn.embedding_lookup(embeddings,tf.argmax(i,axis=1))\n",
    "    gates = tf.matmul(embed,gx)+tf.matmul(o,gm)+gb\n",
    "    state = tf.sigmoid(gates[:,64:128])*state + tf.sigmoid(gates[:,:64])*tf.tanh(gates[:,128:192])\n",
    "    return tf.sigmoid(gates[:,192:]) * tf.tanh(state), state\n",
    "  \n",
    "  #lstm_2\n",
    "  def lstm_cell_2(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    #### one gate input,forget,update,ouput: each 64 in size    \n",
    "    embed = tf.nn.embedding_lookup(embeddings,tf.argmax(i,axis=1))\n",
    "    gates = tf.matmul(embed,gx)+tf.matmul(o,gm)+gb\n",
    "    state = tf.sigmoid(gates[:,64:128])*state + tf.sigmoid(gates[:,:64])*tf.tanh(gates[:,128:192])\n",
    "    return tf.sigmoid(gates[:,192:]) * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(tf.placeholder(tf.int32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM_1 loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "  \n",
    "  #unrolled LSTM_1 loop\n",
    "  outputs = list()\n",
    "  output_2 = saved_output_2\n",
    "  state_2 = state\n",
    "  for i in train_inputs\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.314162 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.50\n",
      "================================================================================\n",
      "nnaeursd dbrtwneoontsmuqopuuuavty k ersotaxh cgemncbofplezexink mjkv toeo k wyxf\n",
      "dvfsmvmrbwzlo evuabmhehwpyfnpixdtbmsebbdzjncct ergl niektgof whresey iqfeena iti\n",
      "goi eamqke aqepmsknbaoccluxvnumahu ltinuhxwgsznjl abnmeeverhdieflhxvokiklccrjeth\n",
      "jhiguzsnwsklfjarr     wseetewhavovlipmcgrkewid wtp ofkeqetwvwnn od ldupinsxe egh\n",
      "erekoedcgwin uippxhjqpmh ftuxbniuoq hzpvse tsqmtahege qedkzuweficbclhao haoyuxao\n",
      "================================================================================\n",
      "Validation set perplexity: 19.68\n",
      "Average loss at step 200: 2.137148 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.97\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 400: 1.927494 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 600: 1.809663 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 5.92\n",
      "Average loss at step 800: 1.792276 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 1000: 1.765994 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1200: 1.714831 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1400: 1.716090 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 1600: 1.687470 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 1800: 1.662429 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2000: 1.662854 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "================================================================================\n",
      "ing oterms daper subscue was of computions exgare seer wron eded evoners widle f\n",
      "ing a south awaq eightahro even or eight two one lisse fle fram a fencher by two\n",
      "c lead for bok one four a the rougher agustar memodity partical earm of whiph ha\n",
      "ed nation the curted four firle portrision woood four a knowsher international l\n",
      "fer of the outhe u pagrhout cal poshondus hada were the ciding eartush project t\n",
      "================================================================================\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 2200: 1.685333 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2400: 1.669305 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2600: 1.663437 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 2800: 1.670228 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 3000: 1.661684 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 3200: 1.635083 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3400: 1.650425 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3600: 1.653532 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3800: 1.655232 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4000: 1.628447 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "================================================================================\n",
      "lown politic f crat trhants the countible foot scomparies countatatistories four\n",
      "k of pronoted be the publically geverally fel afteens cosking to two one nine ei\n",
      "perises of almints jurnesseach x the developication ivaurc presooned is swambs a\n",
      "velter that varies parent thare of channed commentical produck of price definum \n",
      "is privation bich was of recested seven zero s caused jan grobb the myn power in\n",
      "================================================================================\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4200: 1.624680 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4400: 1.634263 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4600: 1.628479 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4800: 1.623951 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5000: 1.625476 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 5200: 1.582386 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.03\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5400: 1.573913 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5600: 1.553830 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5800: 1.570615 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.37\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-1e3d329840ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m       \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     _, l, predictions, lr = session.run(\n\u001b[1;32m---> 15\u001b[1;33m       [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mmean_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msummary_frequency\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rowan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rowan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rowan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rowan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rowan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "summary_frequency = 200\n",
    "time0 = time()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "  print(\"model run time: %0.3fs\"%(time()-time0))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

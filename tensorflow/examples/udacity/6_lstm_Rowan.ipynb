{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "#print(\"ord of a: \",ord('a')-97+1)\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from above:\n",
    "- ' ' = 0\n",
    "- a = 1\n",
    "- b = 2\n",
    "- etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      ##print(\"interupt: \",self._text[self._cursor[b]])\n",
    "      ##print(\"self cursor: \",self._cursor)\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size      \n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    #print(\"interupt: \",np.array(batches).shape)\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    #print(\"interupt: \",np.array(batches).shape)\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    #print(\"interupt: \",s)\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "questions\n",
    "\n",
    "- logprob - why divide by labels.shape i.e. the number of letters\n",
    "- sample distribution makes some sense if np.sum(distribution) = 1\n",
    "- - the larger a probability the higher the liklihood it will push s over r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 200\n",
    "time0 = time()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "  print(\"model run time: %0.3fs\"%(time()-time0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ### all gates\n",
    "  gx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes*4], -0.1, 0.1))\n",
    "  gm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  gb = tf.Variable(tf.zeros([1, num_nodes*4]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  #fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  #fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  #fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  #cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  #cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  #cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  #ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  #om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  #ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    #### one gate input,forget,update,ouput\n",
    "    gates = tf.matmul(i,gx)+tf.matmul(o,gm)+gb\n",
    "    #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    #state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    state = tf.sigmoid(gates[:,64:128])*state + tf.sigmoid(gates[:,:64])*tf.tanh(gates[:,128:192])\n",
    "    return tf.sigmoid(gates[:,192:]) * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 200\n",
    "time0 = time()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "  print(\"model run time: %0.3fs\"%(time()-time0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch2posit(batch):\n",
    "  new_batch =[]  \n",
    "  [new_batch.append(argmax(x)) for x in batch]\n",
    "  return new_batch\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ### all gates\n",
    "  gx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes*4], -0.1, 0.1))\n",
    "  gm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  gb = tf.Variable(tf.zeros([1, num_nodes*4]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  \n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    #### one gate input,forget,update,ouput: each 64 in size\n",
    "    #embed = tf.nn.embedding_lookup(embeddings,batch2posit(i))\n",
    "    #input_data = batch2posit(i)\n",
    "    embed = tf.nn.embedding_lookup(embeddings,tf.argmax(i,axis=1))\n",
    "    gates = tf.matmul(embed,gx)+tf.matmul(o,gm)+gb\n",
    "    state = tf.sigmoid(gates[:,64:128])*state + tf.sigmoid(gates[:,:64])*tf.tanh(gates[:,128:192])\n",
    "    return tf.sigmoid(gates[:,192:]) * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 200\n",
    "time0 = time()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "  print(\"model run time: %0.3fs\"%(time()-time0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2 part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "#print(\"ord of a: \",ord('a')-97+1)\n",
    "\n",
    "def bigram2id(bigram):\n",
    "  return 27*char2id(bigram[0])+char2id(bigram[1])\n",
    "  \n",
    "def id2bigram(dictid):\n",
    "  first = dictid//27\n",
    "  second = dictid%27\n",
    "  return id2char(first)+id2char(second)\n",
    "\n",
    "print(bigram2id('ab'), bigram2id('zg'), bigram2id(' f'), bigram2id('ïg'))\n",
    "print(id2bigram(2), id2bigram(526), id2bigram(728))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=5\n",
    "vocabulary_size = 729\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      ###print(\"self._cursor:\",self._cursor)\n",
    "      ###print(\"b:\", b)\n",
    "      ###print(self._text[self._cursor[b]]+self._text[self._cursor[b]+1])\n",
    "      ###print(bigram2id(self._text[self._cursor[b]]+self._text[self._cursor[b]+1]))\n",
    "      ###print(\"cursor[b+1]:\",self._text[self._cursor[(b+1)% self._text_size]])\n",
    "      batch[b, bigram2id(self._text[self._cursor[b]]+self._text[self._cursor[b]+1])] = 1.0\n",
    "      #print(\"interupt: \",self._text[self._cursor[b]])\n",
    "      self._cursor[b] = (self._cursor[b] + 2) % self._text_size      \n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    #print(\"interupt: \",np.array(batches).shape)\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    #print(\"interupt: \",np.array(batches).shape)\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2bigram(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    #print(\"interupt: \",s)\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ### all gates\n",
    "  gx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes*4], -0.1, 0.1))\n",
    "  gm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  gb = tf.Variable(tf.zeros([1, num_nodes*4]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  \n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    #embed = tf.nn.embedding_lookup(embeddings,batch2posit(i))\n",
    "    #input_data = batch2posit(i)\n",
    "    embed = tf.nn.embedding_lookup(embeddings,tf.argmax(i,axis=1))\n",
    "    gates = tf.matmul(embed,gx)+tf.matmul(o,gm)+gb\n",
    "    state = tf.sigmoid(gates[:,64:128])*state + tf.sigmoid(gates[:,:64])*tf.tanh(gates[:,128:192])\n",
    "    return tf.sigmoid(gates[:,192:]) * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 200\n",
    "time0 = time()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "  print(\"model run time: %0.3fs\"%(time()-time0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3 part 3: implimenting dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "keep_prob = 0.5    \n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ### all gates\n",
    "  gx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes*4], -0.1, 0.1))\n",
    "  gm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  gb = tf.Variable(tf.zeros([1, num_nodes*4]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  \n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    #embed = tf.nn.embedding_lookup(embeddings,batch2posit(i))\n",
    "    #input_data = batch2posit(i)\n",
    "    embed = tf.nn.embedding_lookup(embeddings,tf.argmax(i,axis=1))\n",
    "    embed_dropout = tf.nn.dropout(embed,keep_prob)\n",
    "    gates = tf.matmul(embed,gx)+tf.matmul(o,gm)+gb\n",
    "    state = tf.sigmoid(gates[:,64:128])*state + tf.sigmoid(gates[:,:64])*tf.tanh(gates[:,128:192])\n",
    "    return tf.nn.dropout(tf.sigmoid(gates[:,192:]),keep_prob) * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 200\n",
    "time0 = time()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "  print(\"model run time: %0.3fs\"%(time()-time0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarch', 'when milit', 'lleria arc', ' abbeys an', 'married ur', 'hel and ri', 'y and litu', 'ay opened ', 'tion from ', 'migration ', 'new york o', 'he boeing ', 'e listed w', 'eber has p', 'o be made ', 'yer who re', 'ore signif', 'a fierce c', ' two six e', 'aristotle ', 'ity can be', ' and intra', 'tion of th', 'dy to pass', 'f certain ', 'at it will', 'e convince', 'ent told h', 'ampaign an', 'rver side ', 'ious texts', 'o capitali', 'a duplicat', 'gh ann es ', 'ine januar', 'ross zero ', 'cal theori', 'ast instan', ' dimension', 'most holy ', 't s suppor', 'u is still', 'e oscillat', 'o eight su', 'of italy l', 's the towe', 'klahoma pr', 'erprise li', 'ws becomes', 'et in a na', 'the fabian', 'etchy to r', ' sharman n', 'ised emper', 'ting in po', 'd neo lati', 'th risky r', 'encycloped', 'fense the ', 'duating fr', 'treet grid', 'ations mor', 'appeal of ', 'si have ma']\n",
      "['hists advo', 'tary gover', 'ches natio', 'nd monaste', 'rraca prin', 'ichard bae', 'urgical la', ' for passe', ' the natio', ' took plac', 'other well', ' seven six', 'with a glo', 'probably b', ' to recogn', 'eceived th', 'ficant tha', 'critic of ', 'eight in s', ' s uncause', 'e lost as ', 'acellular ', 'he size of', 's him a st', ' drugs con', 'l take to ', 'e the prie', 'him to nam', 'nd barred ', ' standard ', 's such as ', 'ize on the', 'te of the ', ' d hiver o', 'ry eight m', ' the lead ', 'ies classi', 'nce the no', 'nal analys', ' mormons b', 'rt or at l', 'l disagree', 'ting syste', 'ubtypes ba', 'languages ', 'er commiss', 'ress one n', 'inux suse ', 's the firs', 'azi concen', 'n society ', 'relatively', 'networks s', 'ror hirohi', 'olitical i', 'in most of', 'riskerdoo ', 'dic overvi', ' air compo', 'rom acnm a', 'd centerli', 're than an', ' devotiona', 'ade such d']\n",
      "[' ']\n",
      "[' ']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      ##print(\"interupt: \",self._text[self._cursor[b]])\n",
    "      ##print(\"self cursor: \",self._cursor)\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size      \n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    ##print(\"interupt: \",np.array(batches).shape)\n",
    "    for step in range(self._num_unrollings-1):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    #print(\"interupt: \",np.array(batches).shape)\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    #print(\"interupt: \",s)\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "vocabulary_size = 27\n",
    "num_nodes_1 = 512\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():    \n",
    "\n",
    "\t#Variables\n",
    "\tgx = tf.Variable(tf.truncated_normal([embedding_size,num_nodes_1*4]))\n",
    "\tgb = tf.Variable(tf.truncated_normal([1,num_nodes_1*4]))\n",
    "\n",
    "\tgx_2 = tf.Variable(tf.truncated_normal([num_nodes_1,num_nodes_1*4]))\n",
    "\tgb_2 = tf.Variable(tf.truncated_normal([1,num_nodes_1*4]))\n",
    "\n",
    "\tw = tf.Variable(tf.truncated_normal([num_nodes_1,vocabulary_size]))\n",
    "\tb = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "\tembeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size],-1.0,1.0))\n",
    "\t\n",
    "\t#Placeholders\n",
    "\ttrain_data = list()\n",
    "\tfor _ in range(num_unrollings):\n",
    "\t\ttrain_data.append(tf.placeholder(tf.int32,shape = [batch_size,vocabulary_size]))\n",
    "\t##train_input = train_data[:num_unrollings]\n",
    "\t##train_labels = train_data[1:]\n",
    "\n",
    "\t#Constants #to be placeholders later?\n",
    "\tsaved_output_1 = tf.constant(np.zeros([batch_size,num_nodes_1],dtype = np.float32))\n",
    "\tsaved_state_1 = tf.constant(np.zeros([batch_size,num_nodes_1],dtype = np.float32))\n",
    "\n",
    "\t#lstm\n",
    "\tdef lstm_cell_1(i,state):\n",
    "\t\tembed = tf.nn.embedding_lookup(embeddings,tf.argmax(i,axis=1))\n",
    "\t\tgates = tf.matmul(embed,gx)+gb\n",
    "\t\tstate = tf.sigmoid(gates[:,num_nodes_1:num_nodes_1*2]\n",
    "\t\t\t)*state+tf.sigmoid(gates[:,:num_nodes_1])*tf.tanh(gates[:,num_nodes_1*2:num_nodes_1*3])\n",
    "\t\treturn tf.sigmoid(gates[:,num_nodes_1*3:])*tf.tanh(state),state\n",
    "\n",
    "\tdef lstm_cell_2(o,state):\n",
    "\t\t#embed = tf.nn.embedding_lookup(embeddings,tf.argmax(i,axis=1))\n",
    "\t\tgates = tf.matmul(o,gx_2)+gb_2\n",
    "\t\tstate = tf.sigmoid(gates[:,num_nodes_1:num_nodes_1*2]\n",
    "\t\t\t)*state+tf.sigmoid(gates[:,:num_nodes_1])*tf.tanh(gates[:,num_nodes_1*2:num_nodes_1*3])\n",
    "\t\treturn tf.sigmoid(gates[:,num_nodes_1*3:])*tf.tanh(state),state\n",
    "\n",
    "\t#running through lstm_1\t\n",
    "\tstate = saved_state_1\n",
    "\tfor i in reversed(train_data):\n",
    "\t\t_,state = lstm_cell_1(i,state)\n",
    "\t\t#outputs.append(output)\n",
    "\n",
    "\t#running through lstm_2\n",
    "\toutputs = list()\n",
    "\toutput = saved_output_1\n",
    "\tfor i in range(num_unrollings):\n",
    "\t\toutput,state = lstm_cell_2(output,state)\n",
    "\t\toutputs.append(output)\n",
    "\n",
    "\t#classifier\n",
    "\tlogits = tf.nn.xw_plus_b(tf.concat(outputs,0),w,b)\n",
    "\tloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "\t\tlabels = tf.concat(train_data,0), logits = logits))\n",
    "\n",
    "\t#optimizer\n",
    "\tglobal_step = tf.Variable(0)\n",
    "\tlearning_rate = tf.train.exponential_decay(\n",
    "\t\t10.0,global_step,5000,0.891,staircase=True)\n",
    "\toptimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\tgradients,v = zip(*optimizer.compute_gradients(loss))\n",
    "\tgradients,_ = tf.clip_by_global_norm(gradients,1.25)\n",
    "\toptimizer = optimizer.apply_gradients(zip(gradients,v),global_step = global_step)\n",
    "\n",
    "\t#predictions\n",
    "\ttrain_prediction = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised\n",
      "Average loss at step 200: 13.029896 learning_rate 10.000000\n",
      "Minibatch perplexity: 30888.62\n",
      "Average loss at step 400: 3.387037 learning_rate 10.000000\n",
      "Minibatch perplexity: 9.64\n",
      "Average loss at step 600: 2.300082 learning_rate 10.000000\n",
      "Minibatch perplexity: 8.94\n",
      "==================================================\n",
      "target:  ['e' 'n' 's' 'u' 's' ' ' 'o' 'n' ' ' 't']\n",
      "predic:  ['e' 'n' 't' 'e' 's' 'e' 'e' 'e' 'e' 'e']\n",
      "==================================================\n",
      "target:  ['e' 'n' 't' 'i' 'n' 'a' ' ' 'h' 'a' 'd']\n",
      "predic:  ['e' 'n' 't' 'e' 'e' 'e' 'e' 'e' 'e' 'e']\n",
      "==================================================\n",
      "Average loss at step 800: 2.157414 learning_rate 10.000000\n",
      "Minibatch perplexity: 7.88\n",
      "Average loss at step 1000: 2.089701 learning_rate 10.000000\n",
      "Minibatch perplexity: 7.61\n",
      "Average loss at step 1200: 2.025063 learning_rate 10.000000\n",
      "Minibatch perplexity: 7.08\n",
      "==================================================\n",
      "target:  ['t' 'h' 'e' ' ' 'b' 'o' 'l' 's' 'h' 'e']\n",
      "predic:  ['t' 'h' 'e' ' ' 's' 'a' 'a' ' ' ' ' ' ']\n",
      "==================================================\n",
      "target:  [' ' 'n' 'i' 'n' 'e' ' ' 's' 'e' 'v' 'e']\n",
      "predic:  [' ' 'n' 'i' 'n' 'e' ' ' 'v' 'n' 'n' ' ']\n",
      "==================================================\n",
      "Average loss at step 1400: 1.976713 learning_rate 10.000000\n",
      "Minibatch perplexity: 6.69\n",
      "Average loss at step 1600: 1.944108 learning_rate 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Average loss at step 1800: 1.929221 learning_rate 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "==================================================\n",
      "target:  [' ' 'a' 'n' 'd' ' ' 'v' 'i' 'o' 'l' 'e']\n",
      "predic:  [' ' 'a' 'n' 'd' ' ' 't' 'i' 'n' ' ' ' ']\n",
      "==================================================\n",
      "target:  ['e' 'd' ' ' 'b' 'y' ' ' 's' 'o' 'm' 'e']\n",
      "predic:  ['e' 'd' ' ' 't' 'o' ' ' ' ' ' ' ' ' ' ']\n",
      "==================================================\n",
      "Average loss at step 2000: 1.878586 learning_rate 10.000000\n",
      "Minibatch perplexity: 6.98\n",
      "Average loss at step 2200: 1.860017 learning_rate 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Average loss at step 2400: 1.845269 learning_rate 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "==================================================\n",
      "target:  ['r' 'i' 'c' 'u' 'l' 't' 'u' 'r' 'a' 'l']\n",
      "predic:  ['r' 'i' 'c' 'l' 'l' 'l' ' ' ' ' ' ' ' ']\n",
      "==================================================\n",
      "target:  [' ' 't' 'h' 'e' ' ' 'p' 'r' 'i' 'm' 'e']\n",
      "predic:  [' ' 't' 'h' 'e' ' ' 'p' 'e' 'r' ' ' ' ']\n",
      "==================================================\n",
      "Average loss at step 2600: 1.814126 learning_rate 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Average loss at step 2800: 1.781047 learning_rate 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Average loss at step 3000: 1.770181 learning_rate 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "==================================================\n",
      "target:  ['n' 'd' ' ' 'f' 'a' 'v' 'o' 'r' 's' ' ']\n",
      "predic:  ['n' 'd' ' ' 'b' 'a' ' ' ' ' ' ' ' ' ' ']\n",
      "==================================================\n",
      "target:  ['q' 'u' 'a' 'k' 'e' ' ' 'z' 'o' 'n' 'e']\n",
      "predic:  ['q' 'u' 'a' 't' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      "==================================================\n",
      "Average loss at step 3200: 1.765870 learning_rate 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Average loss at step 3400: 1.742008 learning_rate 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Average loss at step 3600: 1.716379 learning_rate 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "==================================================\n",
      "target:  ['h' 'a' 'v' 'e' ' ' 'c' 'h' 'a' 'r' 'a']\n",
      "predic:  ['h' 'a' 'v' 'e' ' ' 'a' 'a' 'a' 'a' 'a']\n",
      "==================================================\n",
      "target:  ['o' 'r' 'c' 'h' 'e' 's' 't' 'r' 'a' ' ']\n",
      "predic:  ['o' 'r' 'r' 'e' 'e' 'e' ' ' 'e' 'e' 'e']\n",
      "==================================================\n",
      "Average loss at step 3800: 1.707267 learning_rate 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Average loss at step 4000: 1.698723 learning_rate 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Average loss at step 4200: 1.681149 learning_rate 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "==================================================\n",
      "target:  ['y' ' ' 'p' 'o' 'i' 'n' 't' 's' ' ' 'p']\n",
      "predic:  ['y' ' ' 'p' 'o' 'o' 'i' 'o' 'o' 'o' 'o']\n",
      "==================================================\n",
      "target:  ['i' 'm' 'e' ' ' 'c' 'u' 'r' 'r' 'e' 'n']\n",
      "predic:  ['i' 'm' 'e' 'r' 'e' 'e' 'e' 'e' 'e' 'e']\n",
      "==================================================\n",
      "Average loss at step 4400: 1.648758 learning_rate 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Average loss at step 4600: 1.647891 learning_rate 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Average loss at step 4800: 1.635012 learning_rate 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "==================================================\n",
      "target:  ['r' 'l' 'y' ' ' 'i' 'n' 'd' 'i' 'c' 'a']\n",
      "predic:  ['r' 'l' 'y' ' ' ' ' 'a' 'e' 'e' 'e' 'e']\n",
      "==================================================\n",
      "target:  ['e' ' ' 'c' 'l' 'o' 's' 'e' 'd' ' ' 'a']\n",
      "predic:  ['e' ' ' 'c' 'o' 's' ' ' ' ' ' ' ' ' ' ']\n",
      "==================================================\n",
      "Average loss at step 5000: 1.614252 learning_rate 8.910000\n",
      "Minibatch perplexity: 5.46\n",
      "Average loss at step 5200: 1.610910 learning_rate 8.910000\n",
      "Minibatch perplexity: 5.16\n",
      "Average loss at step 5400: 1.585578 learning_rate 8.910000\n",
      "Minibatch perplexity: 4.90\n",
      "==================================================\n",
      "target:  ['t' ' ' 't' 'h' 'e' ' ' 's' 'a' 'm' 'e']\n",
      "predic:  ['t' ' ' 't' 'h' 'e' ' ' 's' 'a' 't' 'h']\n",
      "==================================================\n",
      "target:  ['o' 'p' 'i' 'c' 's' ' ' 'c' 'o' 'm' 'm']\n",
      "predic:  ['o' 'p' 'i' 'i' 'p' 'i' 'i' 'i' 'i' 'i']\n",
      "==================================================\n",
      "Average loss at step 5600: 1.574799 learning_rate 8.910000\n",
      "Minibatch perplexity: 4.98\n",
      "Average loss at step 5800: 1.562404 learning_rate 8.910000\n",
      "Minibatch perplexity: 4.54\n",
      "Average loss at step 6000: 1.552405 learning_rate 8.910000\n",
      "Minibatch perplexity: 4.88\n",
      "==================================================\n",
      "target:  ['g' 'r' 'a' 't' 'e' 'd' ' ' 'i' 'n' 't']\n",
      "predic:  ['g' 'r' 'a' 'a' 't' 'e' 'd' ' ' ' ' 'e']\n",
      "==================================================\n",
      "target:  ['e' ' ' 'r' 'e' 'd' 'u' 'c' 't' 'i' 'o']\n",
      "predic:  ['e' ' ' 'r' 'e' 'r' 't' 't' 'e' 'e' 'e']\n",
      "==================================================\n",
      "Average loss at step 6200: 1.536350 learning_rate 8.910000\n",
      "Minibatch perplexity: 4.31\n",
      "Average loss at step 6400: 1.514185 learning_rate 8.910000\n",
      "Minibatch perplexity: 4.27\n",
      "Average loss at step 6600: 1.507138 learning_rate 8.910000\n",
      "Minibatch perplexity: 4.36\n",
      "==================================================\n",
      "target:  ['i' 'n' 'd' 'i' 'c' 'a' 't' 'i' 'n' 'g']\n",
      "predic:  ['i' 'n' 'd' 'i' 'r' 'i' 'i' ' ' ' ' ' ']\n",
      "==================================================\n",
      "target:  ['e' 'f' 'l' 'e' 'c' 't' 'o' 'r' ' ' 't']\n",
      "predic:  ['e' 'f' 'c' 'l' 'l' 't' ' ' ' ' ' ' ' ']\n",
      "==================================================\n",
      "Average loss at step 6800: 1.523856 learning_rate 8.910000\n",
      "Minibatch perplexity: 4.44\n",
      "Average loss at step 7000: 1.515617 learning_rate 8.910000\n",
      "Minibatch perplexity: 4.27\n",
      "Average loss at step 7200: 1.500838 learning_rate 8.910000\n",
      "Minibatch perplexity: 4.20\n",
      "==================================================\n",
      "target:  ['p' 'h' 'i' 's' 'm' ' ' 'f' 'o' 'r' ' ']\n",
      "predic:  ['p' 'h' 'i' 's' ' ' ' ' ' ' 'o' ' ' ' ']\n",
      "==================================================\n",
      "target:  ['n' ' ' 'o' 'f' ' ' 't' 'h' 'e' ' ' 'c']\n",
      "predic:  ['n' ' ' 'o' 'f' ' ' 't' 'h' 'e' ' ' 'e']\n",
      "==================================================\n",
      "Average loss at step 7400: 1.483259 learning_rate 8.910000\n",
      "Minibatch perplexity: 4.21\n",
      "Average loss at step 7600: 1.480993 learning_rate 8.910000\n",
      "Minibatch perplexity: 4.62\n",
      "Average loss at step 7800: 1.471460 learning_rate 8.910000\n",
      "Minibatch perplexity: 4.37\n",
      "==================================================\n",
      "target:  [' ' 'a' 'u' 't' 'i' 's' 't' 'i' 'c' ' ']\n",
      "predic:  [' ' 'a' 's' 't' 'i' 'c' 't' 'i' 't' 'i']\n",
      "==================================================\n",
      "target:  [' ' 'b' 'y' ' ' 'e' 'n' 't' 'e' 'r' 'i']\n",
      "predic:  [' ' 'b' 'y' ' ' 'e' 'e' ' ' ' ' ' ' ' ']\n",
      "==================================================\n",
      "Average loss at step 8000: 1.471493 learning_rate 8.910000\n",
      "Minibatch perplexity: 4.44\n",
      "Average loss at step 8200: 1.473083 learning_rate 8.910000\n",
      "Minibatch perplexity: 3.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 8400: 1.455866 learning_rate 8.910000\n",
      "Minibatch perplexity: 4.22\n",
      "==================================================\n",
      "target:  [' ' 'a' 'u' 't' 'i' 's' 'm' ' ' 'r' 'e']\n",
      "predic:  [' ' 'a' 'u' 't' 'h' 'n' 't' 't' 't' ' ']\n",
      "==================================================\n",
      "target:  ['i' 'a' 'i' 'd' 'o' ' ' 'j' 'u' 'j' 'u']\n",
      "predic:  ['i' 'a' 'i' 'd' ' ' 'l' 'l' 'l' 'l' 'l']\n",
      "==================================================\n",
      "Average loss at step 8600: 1.450222 learning_rate 8.910000\n",
      "Minibatch perplexity: 4.49\n",
      "Average loss at step 8800: 1.450206 learning_rate 8.910000\n",
      "Minibatch perplexity: 3.86\n",
      "Average loss at step 9000: 1.423087 learning_rate 8.910000\n",
      "Minibatch perplexity: 4.66\n",
      "==================================================\n",
      "target:  ['d' ' ' 'r' 'e' 'f' 'e' 'r' 'e' 'n' 'c']\n",
      "predic:  ['d' ' ' 'r' 'e' 'r' 'e' 'e' ' ' ' ' ' ']\n",
      "==================================================\n",
      "target:  ['i' 'n' 'g' ' ' 't' 'o' ' ' 'n' 'o' 't']\n",
      "predic:  ['i' 'n' 'g' ' ' 'o' 'o' ' ' ' ' 'n' 'e']\n",
      "==================================================\n",
      "Average loss at step 9200: 1.425305 learning_rate 8.910000\n",
      "Minibatch perplexity: 4.16\n",
      "Average loss at step 9400: 1.406436 learning_rate 8.910000\n",
      "Minibatch perplexity: 4.19\n",
      "Average loss at step 9600: 1.417637 learning_rate 8.910000\n",
      "Minibatch perplexity: 3.82\n",
      "==================================================\n",
      "target:  ['a' 'l' 'm' ' ' 'f' 'r' 'o' 'n' 'd' 's']\n",
      "predic:  ['a' 'm' 'm' ' ' 'o' 'o' 'o' ' ' ' ' ' ']\n",
      "==================================================\n",
      "target:  ['i' ' ' 's' 'a' 'i' 'd' ' ' 'h' 'e' ' ']\n",
      "predic:  ['i' ' ' ' ' 'a' 'a' 'd' ' ' ' ' ' ' ' ']\n",
      "==================================================\n",
      "Average loss at step 9800: 1.412225 learning_rate 8.910000\n",
      "Minibatch perplexity: 4.46\n",
      "Average loss at step 10000: 1.399526 learning_rate 7.938809\n",
      "Minibatch perplexity: 4.13\n",
      "Average loss at step 10200: 1.396296 learning_rate 7.938809\n",
      "Minibatch perplexity: 3.85\n",
      "==================================================\n",
      "target:  [' ' 'o' 'n' 'e' ' ' 'z' 'e' 'r' 'o' ' ']\n",
      "predic:  [' ' 'o' 'n' 'e' ' ' 'z' 'e' 'r' 'o' ' ']\n",
      "==================================================\n",
      "target:  [' ' 'd' 'i' 's' 't' 'i' 'n' 'c' 't' 'i']\n",
      "predic:  [' ' 'd' 'i' 's' 't' 'i' 'i' 'i' 'i' 'i']\n",
      "==================================================\n",
      "Average loss at step 10400: 1.374751 learning_rate 7.938809\n",
      "Minibatch perplexity: 3.58\n",
      "Average loss at step 10600: 1.364243 learning_rate 7.938809\n",
      "Minibatch perplexity: 3.90\n",
      "Average loss at step 10800: 1.343923 learning_rate 7.938809\n",
      "Minibatch perplexity: 4.09\n",
      "==================================================\n",
      "target:  ['a' ' ' 'i' 's' ' ' 't' 'h' 'e' ' ' 'c']\n",
      "predic:  ['a' ' ' 'i' 'i' ' ' 't' 'h' 'e' ' ' ' ']\n",
      "==================================================\n",
      "target:  ['e' 'r' 'c' 'i' 's' 'e' 'd' ' ' 'l' 'i']\n",
      "predic:  ['e' 'r' 'i' 'c' 'e' 'e' ' ' ' ' ' ' ' ']\n",
      "==================================================\n",
      "Average loss at step 11000: 1.350158 learning_rate 7.938809\n",
      "Minibatch perplexity: 4.24\n",
      "Average loss at step 11200: 1.344645 learning_rate 7.938809\n",
      "Minibatch perplexity: 3.71\n",
      "Average loss at step 11400: 1.341754 learning_rate 7.938809\n",
      "Minibatch perplexity: 3.73\n",
      "==================================================\n",
      "target:  ['e' ' ' 'c' 'i' 'v' 'i' 'l' ' ' 'r' 'i']\n",
      "predic:  ['e' ' ' 'c' 'i' 'i' 'i' 'i' 'e' 'e' 'e']\n",
      "==================================================\n",
      "target:  [' ' 'i' 'n' ' ' 'd' 'e' 'f' 'i' 'n' 'i']\n",
      "predic:  [' ' 'i' 'n' ' ' 'd' 'n' 'n' 'e' ' ' ' ']\n",
      "==================================================\n",
      "Average loss at step 11600: 1.340067 learning_rate 7.938809\n",
      "Minibatch perplexity: 3.83\n",
      "Average loss at step 11800: 1.310294 learning_rate 7.938809\n",
      "Minibatch perplexity: 3.48\n",
      "Average loss at step 12000: 1.294120 learning_rate 7.938809\n",
      "Minibatch perplexity: 3.50\n",
      "==================================================\n",
      "target:  ['i' 'n' 't' 'e' 'r' 'e' 's' 't' 's' ' ']\n",
      "predic:  ['i' 'n' 't' 'e' 'r' 's' 's' 's' 's' 's']\n",
      "==================================================\n",
      "target:  ['f' 'u' 'n' 'c' 't' 'i' 'o' 'n' 's' ' ']\n",
      "predic:  ['f' 'u' 'n' 'c' 's' 's' ' ' 'o' 'o' 'o']\n",
      "==================================================\n",
      "Average loss at step 12200: 1.310245 learning_rate 7.938809\n",
      "Minibatch perplexity: 3.48\n",
      "Average loss at step 12400: 1.306924 learning_rate 7.938809\n",
      "Minibatch perplexity: 3.89\n",
      "Average loss at step 12600: 1.309730 learning_rate 7.938809\n",
      "Minibatch perplexity: 3.54\n",
      "==================================================\n",
      "target:  [' ' 't' 'h' 'e' ' ' 's' 'c' 'e' 'n' 'e']\n",
      "predic:  [' ' 't' 'h' 'e' ' ' 's' 's' 's' 'e' 'e']\n",
      "==================================================\n",
      "target:  ['i' 'n' 'm' 'e' 'n' 't' ' ' 'a' 'n' 'd']\n",
      "predic:  ['i' 'n' 'e' 'e' 'd' 't' 'a' 'd' 'd' 'd']\n",
      "==================================================\n",
      "Average loss at step 12800: 1.284367 learning_rate 7.938809\n",
      "Minibatch perplexity: 3.75\n",
      "Average loss at step 13000: 1.284739 learning_rate 7.938809\n",
      "Minibatch perplexity: 3.26\n",
      "Average loss at step 13200: 1.278917 learning_rate 7.938809\n",
      "Minibatch perplexity: 3.59\n",
      "==================================================\n",
      "target:  ['d' ' ' 'w' 'i' 't' 'h' ' ' 'g' 'r' 'i']\n",
      "predic:  ['d' ' ' 'w' 'i' 't' 'h' ' ' 'w' 'i' 'g']\n",
      "==================================================\n",
      "target:  ['s' 'b' 'n' ' ' 'z' 'e' 'r' 'o' ' ' 'e']\n",
      "predic:  ['s' 'b' 'n' ' ' 'e' 'e' 'e' ' ' 'e' 'e']\n",
      "==================================================\n",
      "Average loss at step 13400: 1.267778 learning_rate 7.938809\n",
      "Minibatch perplexity: 3.45\n",
      "Average loss at step 13600: 1.264315 learning_rate 7.938809\n",
      "Minibatch perplexity: 3.24\n",
      "Average loss at step 13800: 1.248537 learning_rate 7.938809\n",
      "Minibatch perplexity: 3.54\n",
      "==================================================\n",
      "target:  ['h' 'e' ' ' 'b' 'e' 's' 't' ' ' 'o' 'f']\n",
      "predic:  ['h' 'e' ' ' 'e' 'e' 'f' ' ' ' ' 'o' 'o']\n",
      "==================================================\n",
      "target:  ['d' 'u' 'c' 'a' 't' 'i' 'o' 'n' ' ' 'i']\n",
      "predic:  ['d' 'u' 'c' 't' 'i' 'i' 'n' ' ' ' ' ' ']\n",
      "==================================================\n",
      "Average loss at step 14000: 1.248458 learning_rate 7.938809\n",
      "Minibatch perplexity: 3.28\n",
      "Average loss at step 14200: 1.252065 learning_rate 7.938809\n",
      "Minibatch perplexity: 3.08\n",
      "Average loss at step 14400: 1.231040 learning_rate 7.938809\n",
      "Minibatch perplexity: 3.78\n",
      "==================================================\n",
      "target:  ['a' 's' ' ' 'b' 'o' 'r' 'n' ' ' 'o' 'n']\n",
      "predic:  ['a' 's' ' ' ' ' 'o' 'n' 't' ' ' 'o' 'o']\n",
      "==================================================\n",
      "target:  ['e' ' ' 'd' 'a' 'y' 's' ' ' 'g' 'e' 's']\n",
      "predic:  ['e' ' ' 'd' 'a' 's' ' ' 's' 'e' 's' 'e']\n",
      "==================================================\n",
      "Average loss at step 14600: 1.247982 learning_rate 7.938809\n",
      "Minibatch perplexity: 3.62\n",
      "Average loss at step 14800: 1.240022 learning_rate 7.938809\n",
      "Minibatch perplexity: 3.35\n",
      "Average loss at step 15000: 1.239878 learning_rate 7.073479\n",
      "Minibatch perplexity: 3.34\n",
      "==================================================\n",
      "target:  ['t' ' ' 'h' 'i' 'm' 's' 'e' 'l' 'f' ' ']\n",
      "predic:  ['t' ' ' 'h' 'i' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      "==================================================\n",
      "target:  [' ' 'c' 'o' 'l' 'l' 'e' 'g' 'e' ' ' 'o']\n",
      "predic:  [' ' 'c' 'o' 'l' 'l' 'e' ' ' ' ' 'e' 'e']\n",
      "==================================================\n",
      "Average loss at step 15200: 1.211524 learning_rate 7.073479\n",
      "Minibatch perplexity: 3.41\n",
      "Average loss at step 15400: 1.197566 learning_rate 7.073479\n",
      "Minibatch perplexity: 3.16\n",
      "Average loss at step 15600: 1.214226 learning_rate 7.073479\n",
      "Minibatch perplexity: 3.81\n",
      "==================================================\n",
      "target:  ['n' 'd' ' ' 'o' 'u' 't' ' ' 'a' 'm' 'o']\n",
      "predic:  ['n' 'd' ' ' 'o' 'o' ' ' 'a' 'a' 'a' 't']\n",
      "==================================================\n",
      "target:  ['a' 'r' 'r' 'y' 'i' 'n' 'g' ' ' 'h' 'e']\n",
      "predic:  ['a' 'r' 'r' 'i' 'n' 'n' ' ' 'e' 'e' 'e']\n",
      "==================================================\n",
      "Average loss at step 15800: 1.206871 learning_rate 7.073479\n",
      "Minibatch perplexity: 3.40\n",
      "Average loss at step 16000: 1.195073 learning_rate 7.073479\n",
      "Minibatch perplexity: 3.11\n",
      "Average loss at step 16200: 1.172850 learning_rate 7.073479\n",
      "Minibatch perplexity: 2.89\n",
      "==================================================\n",
      "target:  ['l' 'o' 'u' 'i' 's' 'i' 'a' 'n' 'a' ' ']\n",
      "predic:  ['l' 'o' 'o' 'i' 'i' 'i' 'i' 'i' 'a' 'a']\n",
      "==================================================\n",
      "target:  ['v' 'e' 'n' ' ' 'o' 'n' 'e' ' ' 'z' 'e']\n",
      "predic:  ['v' 'e' 'n' ' ' 'o' 'n' 'e' ' ' 'z' 'e']\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 16400: 1.188888 learning_rate 7.073479\n",
      "Minibatch perplexity: 3.65\n",
      "Average loss at step 16600: 1.173400 learning_rate 7.073479\n",
      "Minibatch perplexity: 3.27\n",
      "Average loss at step 16800: 1.180121 learning_rate 7.073479\n",
      "Minibatch perplexity: 3.67\n",
      "==================================================\n",
      "target:  ['e' 'r' 'e' 't' 'o' ' ' 'e' 's' 'c' 'a']\n",
      "predic:  ['e' 'r' 'e' 'e' 't' ' ' 'o' 'o' 'o' 'n']\n",
      "==================================================\n",
      "target:  ['b' 'i' ' ' 'p' 'o' 'l' 'l' ' ' 'o' 'f']\n",
      "predic:  ['b' 'i' ' ' ' ' 'l' 'l' ' ' ' ' ' ' 'f']\n",
      "==================================================\n",
      "Average loss at step 17000: 1.167537 learning_rate 7.073479\n",
      "Minibatch perplexity: 3.04\n",
      "Average loss at step 17200: 1.165483 learning_rate 7.073479\n",
      "Minibatch perplexity: 3.27\n",
      "Average loss at step 17400: 1.158354 learning_rate 7.073479\n",
      "Minibatch perplexity: 3.42\n",
      "==================================================\n",
      "target:  [' ' 's' 'e' 'i' 'z' 'u' 'r' 'e' 's' ' ']\n",
      "predic:  [' ' 's' 'e' 'r' 'e' 'r' ' ' ' ' ' ' 'e']\n",
      "==================================================\n",
      "target:  ['h' 'e' 'a' 'l' 't' 'h' ' ' 's' 'y' 's']\n",
      "predic:  ['h' 'e' 'a' 't' 'h' 'e' ' ' ' ' 'h' 'h']\n",
      "==================================================\n",
      "Average loss at step 17600: 1.165134 learning_rate 7.073479\n",
      "Minibatch perplexity: 3.58\n",
      "Average loss at step 17800: 1.174298 learning_rate 7.073479\n",
      "Minibatch perplexity: 2.90\n",
      "Average loss at step 18000: 1.169038 learning_rate 7.073479\n",
      "Minibatch perplexity: 3.03\n",
      "==================================================\n",
      "target:  ['n' 'd' ' ' 'c' 'h' 'e' 'r' 'i' 's' 'h']\n",
      "predic:  ['n' 'd' ' ' 'c' 'h' 'c' 'h' 'i' 'i' 'h']\n",
      "==================================================\n",
      "target:  ['t' ' ' 'i' 'n' ' ' 'a' 'n' ' ' 'i' 'n']\n",
      "predic:  ['t' ' ' 'i' 'n' ' ' 'i' 'n' ' ' 'i' 'n']\n",
      "==================================================\n",
      "Average loss at step 18200: 1.152565 learning_rate 7.073479\n",
      "Minibatch perplexity: 3.43\n",
      "Average loss at step 18400: 1.156550 learning_rate 7.073479\n",
      "Minibatch perplexity: 3.37\n",
      "Average loss at step 18600: 1.143057 learning_rate 7.073479\n",
      "Minibatch perplexity: 3.63\n",
      "==================================================\n",
      "target:  ['s' ' ' 'o' 'r' 'a' 't' 'o' 'r' 'i' 'c']\n",
      "predic:  ['s' ' ' 'o' 'r' ' ' 'a' 'a' 'c' 'i' 'i']\n",
      "==================================================\n",
      "target:  ['m' ' ' 'a' ' ' 's' ' ' 'h' ' ' 'f' 'o']\n",
      "predic:  ['m' ' ' 'a' ' ' ' ' 'o' 'f' 'f' ' ' ' ']\n",
      "==================================================\n",
      "Average loss at step 18800: 1.134680 learning_rate 7.073479\n",
      "Minibatch perplexity: 3.47\n",
      "Average loss at step 19000: 1.139766 learning_rate 7.073479\n",
      "Minibatch perplexity: 3.09\n",
      "Average loss at step 19200: 1.129329 learning_rate 7.073479\n",
      "Minibatch perplexity: 3.01\n",
      "==================================================\n",
      "target:  ['n' 'g' 'e' 'r' ' ' 'a' 't' ' ' 'h' 'i']\n",
      "predic:  ['n' 'g' 'e' 'r' ' ' 'h' 'i' 't' 'h' ' ']\n",
      "==================================================\n",
      "target:  ['n' 't' ' ' 'p' 'a' 'r' 't' ' ' 'o' 'f']\n",
      "predic:  ['n' 't' ' ' 'p' 'r' 'r' 't' 'o' ' ' 'o']\n",
      "==================================================\n",
      "Average loss at step 19400: 1.120259 learning_rate 7.073479\n",
      "Minibatch perplexity: 3.09\n",
      "Average loss at step 19600: 1.119603 learning_rate 7.073479\n",
      "Minibatch perplexity: 3.18\n",
      "Average loss at step 19800: 1.112080 learning_rate 7.073479\n",
      "Minibatch perplexity: 2.93\n",
      "==================================================\n",
      "target:  ['o' 'u' 't' 'h' ' ' 'd' 'a' 'k' 'o' 't']\n",
      "predic:  ['o' 'u' 't' 'h' ' ' 't' 't' 't' 't' 't']\n",
      "==================================================\n",
      "target:  ['a' 'l' 'l' ' ' 'b' 'e' 'c' 'o' 'm' 'e']\n",
      "predic:  ['a' 'l' 'l' ' ' 'b' 'e' 'l' 'l' 'e' 'e']\n",
      "==================================================\n",
      "Average loss at step 20000: 1.122318 learning_rate 6.302469\n",
      "Minibatch perplexity: 3.06\n",
      "Average loss at step 20200: 1.096449 learning_rate 6.302469\n",
      "Minibatch perplexity: 3.15\n",
      "Average loss at step 20400: 1.090176 learning_rate 6.302469\n",
      "Minibatch perplexity: 2.70\n",
      "==================================================\n",
      "target:  ['p' 'e' ' ' 'o' 'f' ' ' 'e' 'a' 'r' 't']\n",
      "predic:  ['p' 'e' ' ' 'o' 'f' ' ' 'a' ' ' ' ' ' ']\n",
      "==================================================\n",
      "target:  ['t' 'h' 'e' ' ' 'b' 'a' 'l' 'l' ' ' 'i']\n",
      "predic:  ['t' 'h' 'e' ' ' 'b' 'a' 'l' 'l' ' ' ' ']\n",
      "==================================================\n",
      "Average loss at step 20600: 1.107731 learning_rate 6.302469\n",
      "Minibatch perplexity: 2.88\n",
      "Average loss at step 20800: 1.108504 learning_rate 6.302469\n",
      "Minibatch perplexity: 3.06\n",
      "Average loss at step 21000: 1.095314 learning_rate 6.302469\n",
      "Minibatch perplexity: 3.29\n",
      "==================================================\n",
      "target:  ['e' ' ' 'l' 'i' 'b' 'r' 'a' 'r' 'y' ' ']\n",
      "predic:  ['e' ' ' 'l' 'l' 'l' 'l' ' ' ' ' ' ' ' ']\n",
      "==================================================\n",
      "target:  ['o' 'r' ' ' 't' 'h' 'e' ' ' 'c' 'e' 'n']\n",
      "predic:  ['o' 'r' ' ' 't' 'h' 'e' ' ' 'e' 'e' 'e']\n",
      "==================================================\n",
      "Average loss at step 21200: 1.094437 learning_rate 6.302469\n",
      "Minibatch perplexity: 2.85\n",
      "Average loss at step 21400: 1.076173 learning_rate 6.302469\n",
      "Minibatch perplexity: 3.02\n",
      "Average loss at step 21600: 1.085533 learning_rate 6.302469\n",
      "Minibatch perplexity: 2.99\n",
      "==================================================\n",
      "target:  ['e' 'd' 'o' 'n' ' ' 'i' 't' ' ' 'i' 's']\n",
      "predic:  ['e' 'd' 'o' ' ' 'i' 'n' ' ' 't' ' ' ' ']\n",
      "==================================================\n",
      "target:  ['e' 'n' ' ' 'u' 's' 'e' ' ' 't' 'h' 'e']\n",
      "predic:  ['e' 'n' ' ' 's' 'e' 't' 'h' 'h' 'e' 'e']\n",
      "==================================================\n",
      "Average loss at step 21800: 1.086729 learning_rate 6.302469\n",
      "Minibatch perplexity: 3.14\n",
      "Average loss at step 22000: 1.088179 learning_rate 6.302469\n",
      "Minibatch perplexity: 3.19\n",
      "Average loss at step 22200: 1.085204 learning_rate 6.302469\n",
      "Minibatch perplexity: 2.88\n",
      "==================================================\n",
      "target:  ['i' 'm' 'e' 'd' ' ' 't' 'h' 'a' 't' ' ']\n",
      "predic:  ['i' 'm' 'e' 'd' ' ' 't' 'h' 'a' 't' ' ']\n",
      "==================================================\n",
      "target:  [' ' 'z' 'e' 'r' 'o' ' ' 'y' 'e' 'a' 'r']\n",
      "predic:  [' ' 'z' 'e' 'r' 'o' ' ' 'z' 'e' 'a' 'e']\n",
      "==================================================\n",
      "Average loss at step 22400: 1.077584 learning_rate 6.302469\n",
      "Minibatch perplexity: 3.18\n",
      "Average loss at step 22600: 1.054489 learning_rate 6.302469\n",
      "Minibatch perplexity: 3.08\n",
      "Average loss at step 22800: 1.058674 learning_rate 6.302469\n",
      "Minibatch perplexity: 2.86\n",
      "==================================================\n",
      "target:  ['a' 't' 'e' ' ' 't' 'o' ' ' 's' 'p' 'e']\n",
      "predic:  ['a' 't' 'e' ' ' 't' 'o' ' ' 'm' 'o' 'o']\n",
      "==================================================\n",
      "target:  ['n' 'e' ' ' 's' 'e' 'v' 'e' 'n' ' ' 's']\n",
      "predic:  ['n' 'e' ' ' 's' 'e' 'v' 'e' 'n' ' ' 't']\n",
      "==================================================\n",
      "Average loss at step 23000: 1.071540 learning_rate 6.302469\n",
      "Minibatch perplexity: 2.44\n",
      "Average loss at step 23200: 1.067788 learning_rate 6.302469\n",
      "Minibatch perplexity: 2.68\n",
      "Average loss at step 23400: 1.064903 learning_rate 6.302469\n",
      "Minibatch perplexity: 3.44\n",
      "==================================================\n",
      "target:  ['t' 'h' 'e' ' ' 'p' 'l' 'a' 'n' ' ' 't']\n",
      "predic:  ['t' 'h' 'e' ' ' 'p' 'e' 'a' 'o' 'o' ' ']\n",
      "==================================================\n",
      "target:  ['r' 'o' ' ' 'z' 'e' 'r' 'o' ' ' 'z' 'e']\n",
      "predic:  ['r' 'o' ' ' 'z' 'e' 'r' 'o' ' ' 'z' 'e']\n",
      "==================================================\n",
      "Average loss at step 23600: 1.055732 learning_rate 6.302469\n",
      "Minibatch perplexity: 2.86\n",
      "Average loss at step 23800: 1.055935 learning_rate 6.302469\n",
      "Minibatch perplexity: 2.59\n",
      "Average loss at step 24000: 1.055635 learning_rate 6.302469\n",
      "Minibatch perplexity: 2.92\n",
      "==================================================\n",
      "target:  ['r' 'i' 's' 't' 'o' 't' 'l' 'e' ' ' 'h']\n",
      "predic:  ['r' 'i' 's' 't' 't' ' ' 't' 'h' 'e' 'e']\n",
      "==================================================\n",
      "target:  ['y' ' ' 'm' 'e' 'n' ' ' 'w' 'e' 'r' 'e']\n",
      "predic:  ['y' ' ' 'm' 'e' 'n' 'e' 'e' 'e' 'e' 'e']\n",
      "==================================================\n",
      "Average loss at step 24200: 1.051469 learning_rate 6.302469\n",
      "Minibatch perplexity: 3.03\n",
      "Average loss at step 24400: 1.044156 learning_rate 6.302469\n",
      "Minibatch perplexity: 2.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 24600: 1.048691 learning_rate 6.302469\n",
      "Minibatch perplexity: 2.73\n",
      "==================================================\n",
      "target:  ['h' 'o' 'd' 'e' 's' ' ' 'b' 'i' 'b' 'l']\n",
      "predic:  ['h' 'o' 'd' 'd' 'e' 'b' ' ' 'b' 'b' 'b']\n",
      "==================================================\n",
      "target:  ['a' 'n' ' ' 'i' 'n' 'd' 'i' 'a' 'n' 's']\n",
      "predic:  ['a' 'n' ' ' 'a' 'n' 'd' 'i' 'i' 'n' 'n']\n",
      "==================================================\n",
      "Average loss at step 24800: 1.054947 learning_rate 6.302469\n",
      "Minibatch perplexity: 2.70\n",
      "Average loss at step 25000: 1.036798 learning_rate 5.615500\n",
      "Minibatch perplexity: 3.09\n",
      "Average loss at step 25200: 1.030982 learning_rate 5.615500\n",
      "Minibatch perplexity: 2.93\n",
      "==================================================\n",
      "target:  ['s' 'e' 'n' 't' 'e' 'd' ' ' 'o' 'n' 'e']\n",
      "predic:  ['s' 'e' 'n' 't' 'a' 'd' ' ' 'o' 'n' 'e']\n",
      "==================================================\n",
      "target:  [' ' 'w' 'e' 'r' 'e' ' ' 'b' 'a' 'c' 'k']\n",
      "predic:  [' ' 'w' 'e' 'r' 'e' ' ' 'b' 'a' 'a' 'a']\n",
      "==================================================\n",
      "Average loss at step 25400: 1.023676 learning_rate 5.615500\n",
      "Minibatch perplexity: 2.74\n",
      "Average loss at step 25600: 1.036266 learning_rate 5.615500\n",
      "Minibatch perplexity: 2.53\n",
      "Average loss at step 25800: 1.017356 learning_rate 5.615500\n",
      "Minibatch perplexity: 2.56\n",
      "==================================================\n",
      "target:  ['r' 't' ' ' 's' 'u' 'b' 'j' 'e' 'c' 't']\n",
      "predic:  ['r' 't' ' ' 's' 't' 'b' 'b' 'r' 'a' 'c']\n",
      "==================================================\n",
      "target:  ['r' 'y' ' ' 'h' 'i' 's' 't' 'o' 'r' 'y']\n",
      "predic:  ['r' 'y' ' ' 'k' 'i' 'r' 't' 'o' 't' 'c']\n",
      "==================================================\n",
      "Average loss at step 26000: 1.026342 learning_rate 5.615500\n",
      "Minibatch perplexity: 2.97\n",
      "Average loss at step 26200: 1.021582 learning_rate 5.615500\n",
      "Minibatch perplexity: 2.83\n",
      "Average loss at step 26400: 1.016656 learning_rate 5.615500\n",
      "Minibatch perplexity: 2.41\n",
      "==================================================\n",
      "target:  ['p' 'e' 'r' 'i' 'o' 'd' 'i' 'c' 'a' 'l']\n",
      "predic:  ['p' 'e' 'r' 'i' 'o' 'n' 'i' 't' 'i' 'd']\n",
      "==================================================\n",
      "target:  [' ' 'w' 'h' 'i' 'c' 'h' ' ' 'h' 'a' 'd']\n",
      "predic:  [' ' 'w' 'h' 'i' 'c' 'h' ' ' 'a' 'a' 's']\n",
      "==================================================\n",
      "Average loss at step 26600: 1.010738 learning_rate 5.615500\n",
      "Minibatch perplexity: 2.92\n",
      "Average loss at step 26800: 0.999472 learning_rate 5.615500\n",
      "Minibatch perplexity: 2.88\n",
      "Average loss at step 27000: 0.981916 learning_rate 5.615500\n",
      "Minibatch perplexity: 2.63\n",
      "==================================================\n",
      "target:  ['t' 'h' 'a' 't' ' ' 's' 'h' 'o' 'w' 'e']\n",
      "predic:  ['t' 'h' 'a' 't' ' ' 'w' 'w' 'o' ' ' 'u']\n",
      "==================================================\n",
      "target:  ['t' ' ' 's' 'u' 'r' 'v' 'i' 'v' 'i' 'n']\n",
      "predic:  ['t' ' ' 's' 'u' 'v' 'v' 'n' 'n' 'e' 't']\n",
      "==================================================\n",
      "Average loss at step 27200: 0.994417 learning_rate 5.615500\n",
      "Minibatch perplexity: 3.04\n",
      "Average loss at step 27400: 0.991692 learning_rate 5.615500\n",
      "Minibatch perplexity: 2.70\n",
      "Average loss at step 27600: 0.980167 learning_rate 5.615500\n",
      "Minibatch perplexity: 2.62\n",
      "==================================================\n",
      "target:  ['t' 'r' 'u' 'i' 's' 't' ' ' 'i' 'n' ' ']\n",
      "predic:  ['t' 'r' 'i' 'c' 't' 'i' ' ' 'n' ' ' 'o']\n",
      "==================================================\n",
      "target:  ['a' 'm' 'p' ' ' 'r' 'e' ' ' 's' 'e' 'e']\n",
      "predic:  ['a' 'm' 'p' ' ' 'e' 'e' 'e' 'e' ' ' ' ']\n",
      "==================================================\n",
      "Average loss at step 27800: 0.985212 learning_rate 5.615500\n",
      "Minibatch perplexity: 2.66\n",
      "Average loss at step 28000: 0.995292 learning_rate 5.615500\n",
      "Minibatch perplexity: 2.80\n",
      "Average loss at step 28200: 1.004320 learning_rate 5.615500\n",
      "Minibatch perplexity: 2.76\n",
      "==================================================\n",
      "target:  ['n' 'o' 't' ' ' 't' 'h' 'e' ' ' 'c' 'a']\n",
      "predic:  ['n' 'o' 't' ' ' 'c' 'e' 'c' 't' ' ' 'c']\n",
      "==================================================\n",
      "target:  ['n' 't' 'e' 'r' ' ' 'c' 'h' 'a' 'i' 'n']\n",
      "predic:  ['n' 't' 'e' 'r' ' ' ' ' 'c' 'a' 'c' 'i']\n",
      "==================================================\n",
      "Average loss at step 28400: 0.991487 learning_rate 5.615500\n",
      "Minibatch perplexity: 2.50\n",
      "Average loss at step 28600: 0.977715 learning_rate 5.615500\n",
      "Minibatch perplexity: 2.57\n",
      "Average loss at step 28800: 0.969483 learning_rate 5.615500\n",
      "Minibatch perplexity: 2.68\n",
      "==================================================\n",
      "target:  [' ' 'm' 'o' 'r' 'a' 'l' ' ' 'o' 'b' 'l']\n",
      "predic:  [' ' 'm' 'o' 'r' 'a' ' ' 'a' 'a' 'p' 'a']\n",
      "==================================================\n",
      "target:  [' ' 's' 'h' 'i' 'r' 't' ' ' 'd' 'i' 's']\n",
      "predic:  [' ' 's' 'h' 'i' 't' 't' 'i' 'n' 'n' ' ']\n",
      "==================================================\n",
      "Average loss at step 29000: 0.974508 learning_rate 5.615500\n",
      "Minibatch perplexity: 2.53\n",
      "Average loss at step 29200: 0.987690 learning_rate 5.615500\n",
      "Minibatch perplexity: 2.76\n",
      "Average loss at step 29400: 0.954549 learning_rate 5.615500\n",
      "Minibatch perplexity: 2.52\n",
      "==================================================\n",
      "target:  ['r' 'e' ' ' 'b' 'm' 'w' ' ' 's' 'h' 'o']\n",
      "predic:  ['r' 'e' ' ' 's' 'p' 'o' 'o' ' ' ' ' ' ']\n",
      "==================================================\n",
      "target:  ['a' ' ' 't' 'e' 'a' 'm' ' ' 't' 'r' 'a']\n",
      "predic:  ['a' ' ' 't' 'a' 't' 't' 'a' 't' 't' 't']\n",
      "==================================================\n",
      "Average loss at step 29600: 0.977078 learning_rate 5.615500\n",
      "Minibatch perplexity: 2.52\n",
      "Average loss at step 29800: 0.972248 learning_rate 5.615500\n",
      "Minibatch perplexity: 2.34\n",
      "Average loss at step 30000: 0.968829 learning_rate 5.003410\n",
      "Minibatch perplexity: 2.38\n",
      "==================================================\n",
      "target:  ['n' 'g' ' ' 't' 'h' 'e' ' ' 'n' 'a' 'm']\n",
      "predic:  ['n' 'g' ' ' 't' 'h' 'e' ' ' 'a' 'a' 'm']\n",
      "==================================================\n",
      "target:  [' ' 's' 'o' 'm' 'e' ' ' 'f' 'o' 'r' 'm']\n",
      "predic:  [' ' 's' 'o' 'm' 'e' ' ' 'o' 'o' 'm' 'm']\n",
      "==================================================\n",
      "Average loss at step 30200: 0.944845 learning_rate 5.003410\n",
      "Minibatch perplexity: 2.57\n",
      "Average loss at step 30400: 0.964387 learning_rate 5.003410\n",
      "Minibatch perplexity: 2.57\n",
      "Average loss at step 30600: 0.954648 learning_rate 5.003410\n",
      "Minibatch perplexity: 2.63\n",
      "==================================================\n",
      "target:  ['i' 'n' 't' 's' ' ' 'w' 'a' 's' ' ' 'n']\n",
      "predic:  ['i' 'n' 't' 's' ' ' 's' 'a' 's' ' ' 's']\n",
      "==================================================\n",
      "target:  ['i' 's' ' ' 'p' 'r' 'o' 't' 'e' 'c' 't']\n",
      "predic:  ['i' 's' ' ' 'p' 'r' 'o' 't' 'o' 'c' 'c']\n",
      "==================================================\n",
      "Average loss at step 30800: 0.948703 learning_rate 5.003410\n",
      "Minibatch perplexity: 2.35\n",
      "Average loss at step 31000: 0.948249 learning_rate 5.003410\n",
      "Minibatch perplexity: 2.59\n",
      "Average loss at step 31200: 0.945634 learning_rate 5.003410\n",
      "Minibatch perplexity: 2.85\n",
      "==================================================\n",
      "target:  ['l' 'y' ' ' 'w' 'a' 's' ' ' 'a' 'p' 'p']\n",
      "predic:  ['l' 'y' ' ' 'w' 'a' 's' ' ' ' ' 'p' 'a']\n",
      "==================================================\n",
      "target:  ['d' ' ' 'i' 'm' 'm' 'e' 'd' 'i' 'a' 't']\n",
      "predic:  ['d' ' ' 'i' 'i' 'm' 'i' 't' 't' 't' 't']\n",
      "==================================================\n",
      "Average loss at step 31400: 0.944984 learning_rate 5.003410\n",
      "Minibatch perplexity: 2.48\n",
      "Average loss at step 31600: 0.939911 learning_rate 5.003410\n",
      "Minibatch perplexity: 2.73\n",
      "Average loss at step 31800: 0.928690 learning_rate 5.003410\n",
      "Minibatch perplexity: 2.33\n",
      "==================================================\n",
      "target:  ['y' ' ' 't' 'o' ' ' 'c' 'o' 'p' 'e' ' ']\n",
      "predic:  ['y' ' ' 't' 'o' ' ' 'p' 'o' ' ' ' ' ' ']\n",
      "==================================================\n",
      "target:  ['n' ' ' 't' 'y' 'p' 'i' 'c' 'a' 'l' 'l']\n",
      "predic:  ['n' ' ' 't' 'y' 'p' 'l' 'l' 'l' 'l' 'l']\n",
      "==================================================\n",
      "Average loss at step 32000: 0.924432 learning_rate 5.003410\n",
      "Minibatch perplexity: 2.42\n",
      "Average loss at step 32200: 0.948987 learning_rate 5.003410\n",
      "Minibatch perplexity: 2.85\n",
      "Average loss at step 32400: 0.935104 learning_rate 5.003410\n",
      "Minibatch perplexity: 2.26\n",
      "==================================================\n",
      "target:  [' ' 'o' 'f' ' ' 'd' 'e' 'l' 'i' 'v' 'e']\n",
      "predic:  [' ' 'o' 'f' ' ' 'd' 'e' 'v' 'e' 'e' 'e']\n",
      "==================================================\n",
      "target:  [' ' 'c' 'o' 'm' 'p' 'u' 't' 'e' 'r' 's']\n",
      "predic:  [' ' 'c' 'o' 'm' 'p' 'u' 't' 't' 'e' 'e']\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 32600: 0.946145 learning_rate 5.003410\n",
      "Minibatch perplexity: 2.65\n",
      "Average loss at step 32800: 0.917886 learning_rate 5.003410\n",
      "Minibatch perplexity: 2.24\n",
      "Average loss at step 33000: 0.919397 learning_rate 5.003410\n",
      "Minibatch perplexity: 2.64\n",
      "==================================================\n",
      "target:  [' ' 'o' 'n' 'e' ' ' 'n' 'i' 'n' 'e' ' ']\n",
      "predic:  [' ' 'o' 'n' 'e' ' ' 'n' 'i' 'n' 'e' ' ']\n",
      "==================================================\n",
      "target:  ['e' 't' 'e' ' ' 't' 'h' 'e' ' ' 's' 'a']\n",
      "predic:  ['e' 't' 'e' ' ' 't' 'h' 'e' ' ' 'e' 'a']\n",
      "==================================================\n",
      "Average loss at step 33200: 0.925339 learning_rate 5.003410\n",
      "Minibatch perplexity: 2.57\n",
      "Average loss at step 33400: 0.901613 learning_rate 5.003410\n",
      "Minibatch perplexity: 2.56\n",
      "Average loss at step 33600: 0.911170 learning_rate 5.003410\n",
      "Minibatch perplexity: 2.33\n",
      "==================================================\n",
      "target:  [' ' 't' 'h' 'e' ' ' 'g' 'o' 'v' 'e' 'r']\n",
      "predic:  [' ' 't' 'h' 'e' ' ' 'f' 'o' 'v' 'e' 'e']\n",
      "==================================================\n",
      "target:  ['i' 'n' 'd' 'e' 'p' 'e' 'n' 'd' 'e' 'n']\n",
      "predic:  ['i' 'n' 'd' 'e' 'd' 'e' 'n' 'd' 'e' 'n']\n",
      "==================================================\n",
      "Average loss at step 33800: 0.895614 learning_rate 5.003410\n",
      "Minibatch perplexity: 2.53\n",
      "Average loss at step 34000: 0.915753 learning_rate 5.003410\n",
      "Minibatch perplexity: 2.34\n",
      "Average loss at step 34200: 0.906451 learning_rate 5.003410\n",
      "Minibatch perplexity: 2.50\n",
      "==================================================\n",
      "target:  ['e' 't' 'r' 'o' 'l' 'e' 'u' 'm' ' ' 'a']\n",
      "predic:  ['e' 't' 'r' 'o' 'l' 'e' 'l' 'l' 'l' 'm']\n",
      "==================================================\n",
      "target:  ['c' 't' 'u' 'r' 'e' ' ' 'r' 'a' 'n' 'd']\n",
      "predic:  ['c' 't' 'u' 'r' 'e' ' ' 'a' 'n' 'd' 'd']\n",
      "==================================================\n",
      "Average loss at step 34400: 0.899710 learning_rate 5.003410\n",
      "Minibatch perplexity: 2.12\n",
      "Average loss at step 34600: 0.884736 learning_rate 5.003410\n",
      "Minibatch perplexity: 2.16\n",
      "Average loss at step 34800: 0.910012 learning_rate 5.003410\n",
      "Minibatch perplexity: 2.48\n",
      "==================================================\n",
      "target:  ['w' 'a' 'r' ' ' 'o' 'f' ' ' 'i' 'n' 'd']\n",
      "predic:  ['w' 'a' 'r' ' ' 'o' 'o' ' ' ' ' 'n' 't']\n",
      "==================================================\n",
      "target:  ['t' 'a' 'l' ' ' 'e' 'q' 'u' 'i' 'p' 'm']\n",
      "predic:  ['t' 'a' 'l' ' ' 'l' 'r' 't' 'r' 'r' 'r']\n",
      "==================================================\n",
      "Average loss at step 35000: 0.902377 learning_rate 4.458039\n",
      "Minibatch perplexity: 2.28\n",
      "Average loss at step 35200: 0.875830 learning_rate 4.458039\n",
      "Minibatch perplexity: 2.41\n",
      "Average loss at step 35400: 0.871472 learning_rate 4.458039\n",
      "Minibatch perplexity: 2.65\n",
      "==================================================\n",
      "target:  ['u' 'e' 's' ' ' 'r' 'e' 'l' 'a' 't' 'i']\n",
      "predic:  ['u' 'e' 's' ' ' 's' 'e' 'l' 'a' 'a' 'l']\n",
      "==================================================\n",
      "target:  ['i' 'r' ' ' 'f' 'u' 'l' 'l' ' ' 'i' 'n']\n",
      "predic:  ['i' 'r' ' ' 'f' 'f' 'l' 'l' ' ' 'i' 'l']\n",
      "==================================================\n",
      "Average loss at step 35600: 0.876584 learning_rate 4.458039\n",
      "Minibatch perplexity: 2.29\n",
      "Average loss at step 35800: 0.875864 learning_rate 4.458039\n",
      "Minibatch perplexity: 2.28\n",
      "Average loss at step 36000: 0.870684 learning_rate 4.458039\n",
      "Minibatch perplexity: 2.60\n",
      "==================================================\n",
      "target:  ['i' 'n' 't' 'i' 'n' 'g' ' ' 'm' 'o' 'h']\n",
      "predic:  ['i' 'n' 't' 'i' 'i' 'n' ' ' ' ' 'o' 'o']\n",
      "==================================================\n",
      "target:  [' ' 'm' 'u' 'r' 'd' 'e' 'r' 'e' 'd' ' ']\n",
      "predic:  [' ' 'm' 'u' 'r' 'd' ' ' 'd' 'e' 'r' 'd']\n",
      "==================================================\n",
      "Average loss at step 36200: 0.866699 learning_rate 4.458039\n",
      "Minibatch perplexity: 2.35\n",
      "Average loss at step 36400: 0.867363 learning_rate 4.458039\n",
      "Minibatch perplexity: 2.31\n",
      "Average loss at step 36600: 0.856042 learning_rate 4.458039\n",
      "Minibatch perplexity: 2.11\n",
      "==================================================\n",
      "target:  ['c' 'o' 'n' 't' 'r' 'o' 'l' ' ' 'o' 'v']\n",
      "predic:  ['c' 'o' 'n' 't' 'r' 'o' ' ' ' ' 'o' 'o']\n",
      "==================================================\n",
      "target:  ['f' ' ' 's' 'h' 'a' 'h' 'a' 'n' 's' 'h']\n",
      "predic:  ['f' ' ' 's' 'h' 'a' 'h' 'h' 'h' 'h' 'h']\n",
      "==================================================\n",
      "Average loss at step 36800: 0.863898 learning_rate 4.458039\n",
      "Minibatch perplexity: 2.34\n",
      "Average loss at step 37000: 0.844243 learning_rate 4.458039\n",
      "Minibatch perplexity: 2.45\n",
      "Average loss at step 37200: 0.855493 learning_rate 4.458039\n",
      "Minibatch perplexity: 2.13\n",
      "==================================================\n",
      "target:  ['m' ' ' 't' 'h' 'e' ' ' 'r' 'e' 'a' 'd']\n",
      "predic:  ['m' ' ' 't' 'h' 'e' ' ' 'r' 'e' 'a' 'r']\n",
      "==================================================\n",
      "target:  ['i' 'e' 'l' 'd' ' ' 'u' 's' 'e' 'd' ' ']\n",
      "predic:  ['i' 'e' 'l' 'd' ' ' 's' 'e' 's' 'e' ' ']\n",
      "==================================================\n",
      "Average loss at step 37400: 0.851500 learning_rate 4.458039\n",
      "Minibatch perplexity: 2.33\n",
      "Average loss at step 37600: 0.850474 learning_rate 4.458039\n",
      "Minibatch perplexity: 2.18\n",
      "Average loss at step 37800: 0.857562 learning_rate 4.458039\n",
      "Minibatch perplexity: 2.19\n",
      "==================================================\n",
      "target:  ['e' ' ' 'w' 'h' 'i' 'c' 'h' ' ' 's' 'h']\n",
      "predic:  ['e' ' ' 'w' 'h' 'i' 'c' 'h' ' ' 'h' 'h']\n",
      "==================================================\n",
      "target:  ['a' 'l' 'e' 'x' 'a' 'n' 'd' 'e' 'r' ' ']\n",
      "predic:  ['a' 'l' 'e' 'x' 'a' 'n' 'd' ' ' ' ' ' ']\n",
      "==================================================\n",
      "Average loss at step 38000: 0.842130 learning_rate 4.458039\n",
      "Minibatch perplexity: 2.45\n",
      "Average loss at step 38200: 0.830035 learning_rate 4.458039\n",
      "Minibatch perplexity: 2.27\n",
      "Average loss at step 38400: 0.851983 learning_rate 4.458039\n",
      "Minibatch perplexity: 2.28\n",
      "==================================================\n",
      "target:  ['p' 'a' 'u' 'l' ' ' 'l' 'a' 'r' 'k' 'i']\n",
      "predic:  ['p' 'a' 'l' 'l' ' ' 'a' 'i' 'l' 'i' 'i']\n",
      "==================================================\n",
      "target:  ['x' 'a' 'n' 'd' 'e' 'r' ' ' 't' 'h' 'e']\n",
      "predic:  ['x' 'a' 'n' 'd' 'e' 'd' ' ' 't' 'h' 'e']\n",
      "==================================================\n",
      "Average loss at step 38600: 0.841977 learning_rate 4.458039\n",
      "Minibatch perplexity: 2.40\n",
      "Average loss at step 38800: 0.841509 learning_rate 4.458039\n",
      "Minibatch perplexity: 2.20\n",
      "Average loss at step 39000: 0.835564 learning_rate 4.458039\n",
      "Minibatch perplexity: 2.75\n",
      "==================================================\n",
      "target:  ['o' 't' 'e' 'r' 's' ' ' 'a' 'n' 'd' ' ']\n",
      "predic:  ['o' 't' 't' 'r' 's' 's' ' ' 'a' 'n' 'd']\n",
      "==================================================\n",
      "target:  [' ' 't' 'h' 'e' ' ' 'p' 'r' 'o' 'p' 'h']\n",
      "predic:  [' ' 't' 'h' 'e' ' ' 'p' 'r' 'o' 'p' 'i']\n",
      "==================================================\n",
      "Average loss at step 39200: 0.837617 learning_rate 4.458039\n",
      "Minibatch perplexity: 2.82\n",
      "Average loss at step 39400: 0.838454 learning_rate 4.458039\n",
      "Minibatch perplexity: 2.24\n",
      "Average loss at step 39600: 0.825071 learning_rate 4.458039\n",
      "Minibatch perplexity: 2.15\n",
      "==================================================\n",
      "target:  ['r' 'a' 't' 'i' 'o' 'n' 's' ' ' 'd' 'a']\n",
      "predic:  ['r' 'a' 't' 'i' 'o' 'n' ' ' 'a' 'n' 'd']\n",
      "==================================================\n",
      "target:  ['t' 'h' 'e' ' ' 'a' 'l' 'e' 'x' 'a' 'n']\n",
      "predic:  ['t' 'h' 'e' ' ' 'a' 'l' 'l' 'a' 'a' 'l']\n",
      "==================================================\n",
      "Average loss at step 39800: 0.838852 learning_rate 4.458039\n",
      "Minibatch perplexity: 2.21\n",
      "Average loss at step 40000: 0.832813 learning_rate 3.972112\n",
      "Minibatch perplexity: 2.41\n",
      "Average loss at step 40200: 0.814903 learning_rate 3.972112\n",
      "Minibatch perplexity: 2.27\n",
      "==================================================\n",
      "target:  ['w' 'y' 'a' 't' 't' ' ' 'o' 'i' 'l' ' ']\n",
      "predic:  ['w' 'y' 't' 't' 'i' ' ' ' ' 'o' 'o' 'o']\n",
      "==================================================\n",
      "target:  [' ' 's' ' ' 'd' 'e' 'a' 't' 'h' ' ' 'f']\n",
      "predic:  [' ' 's' ' ' 'd' ' ' 'a' 't' 'h' ' ' ' ']\n",
      "==================================================\n",
      "Average loss at step 40400: 0.806898 learning_rate 3.972112\n",
      "Minibatch perplexity: 2.07\n",
      "Average loss at step 40600: 0.820858 learning_rate 3.972112\n",
      "Minibatch perplexity: 2.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 40800: 0.815646 learning_rate 3.972112\n",
      "Minibatch perplexity: 2.51\n",
      "==================================================\n",
      "target:  ['t' 'h' 'e' ' ' 'b' 'r' 'a' 'c' 'e' 'l']\n",
      "predic:  ['t' 'h' 'e' ' ' 'b' 'r' 'a' 'c' 'c' 'y']\n",
      "==================================================\n",
      "target:  ['i' ' ' 'o' 'n' 'e' ' ' 'd' 'a' 'y' ' ']\n",
      "predic:  ['i' ' ' 'o' 'n' 'e' ' ' 'a' 'e' 'e' ' ']\n",
      "==================================================\n",
      "Average loss at step 41000: 0.813317 learning_rate 3.972112\n",
      "Minibatch perplexity: 2.33\n",
      "Average loss at step 41200: 0.819583 learning_rate 3.972112\n",
      "Minibatch perplexity: 2.25\n",
      "Average loss at step 41400: 0.810818 learning_rate 3.972112\n",
      "Minibatch perplexity: 2.18\n",
      "==================================================\n",
      "target:  ['a' 'n' 't' 'i' 's' ' ' 'w' 'h' 'i' 'l']\n",
      "predic:  ['a' 'n' 't' 'i' 'i' 'w' 'i' 'i' 'i' 'i']\n",
      "==================================================\n",
      "target:  ['g' ' ' 's' 'p' 'e' 'e' 'd' 's' ' ' 'p']\n",
      "predic:  ['g' ' ' 's' 'p' 'e' 'e' 'e' ' ' 'p' 'p']\n",
      "==================================================\n",
      "Average loss at step 41600: 0.806084 learning_rate 3.972112\n",
      "Minibatch perplexity: 2.44\n",
      "Average loss at step 41800: 0.809958 learning_rate 3.972112\n",
      "Minibatch perplexity: 2.32\n",
      "Average loss at step 42000: 0.808251 learning_rate 3.972112\n",
      "Minibatch perplexity: 2.10\n",
      "==================================================\n",
      "target:  [' ' 'h' 'e' 'a' 'd' 'q' 'u' 'a' 'r' 't']\n",
      "predic:  [' ' 'h' 'e' 'a' 'd' 'd' 'u' 'a' 'a' 'a']\n",
      "==================================================\n",
      "target:  ['i' 'd' 's' ' ' 'w' 'a' 's' ' ' 'd' 'e']\n",
      "predic:  ['i' 'd' 's' ' ' 'w' 'a' 's' ' ' 'e' 's']\n",
      "==================================================\n",
      "Average loss at step 42200: 0.802192 learning_rate 3.972112\n",
      "Minibatch perplexity: 2.11\n",
      "Average loss at step 42400: 0.795158 learning_rate 3.972112\n",
      "Minibatch perplexity: 2.20\n",
      "Average loss at step 42600: 0.800340 learning_rate 3.972112\n",
      "Minibatch perplexity: 2.01\n",
      "==================================================\n",
      "target:  ['l' 'e' ' ' 'c' 'a' 's' 'u' 'a' 'l' 'l']\n",
      "predic:  ['l' 'e' ' ' 'c' 'a' 'l' 'h' 'a' 'a' 'l']\n",
      "==================================================\n",
      "target:  ['a' ' ' 'g' 'r' 'o' 'u' 'p' ' ' 'o' 'f']\n",
      "predic:  ['a' ' ' 'g' 'o' 'r' 'm' 'o' ' ' 'o' 'o']\n",
      "==================================================\n",
      "Average loss at step 42800: 0.787301 learning_rate 3.972112\n",
      "Minibatch perplexity: 2.32\n",
      "Average loss at step 43000: 0.776509 learning_rate 3.972112\n",
      "Minibatch perplexity: 2.19\n",
      "Average loss at step 43200: 0.772494 learning_rate 3.972112\n",
      "Minibatch perplexity: 2.31\n",
      "==================================================\n",
      "target:  ['e' 'n' 'd' 'e' 'n' 'c' 'e' ' ' 'a' 'n']\n",
      "predic:  ['e' 'n' 'd' 'e' 'n' 'c' 'e' ' ' 'a' 'a']\n",
      "==================================================\n",
      "target:  ['d' 's' ' ' 'a' 'r' 'e' ' ' 'b' 'e' 'l']\n",
      "predic:  ['d' 's' ' ' 'a' 'r' ' ' 'e' 'e' 'e' 'e']\n",
      "==================================================\n",
      "Average loss at step 43400: 0.762597 learning_rate 3.972112\n",
      "Minibatch perplexity: 2.03\n",
      "Average loss at step 43600: 0.766270 learning_rate 3.972112\n",
      "Minibatch perplexity: 2.11\n",
      "Average loss at step 43800: 0.757297 learning_rate 3.972112\n",
      "Minibatch perplexity: 2.07\n",
      "==================================================\n",
      "target:  [' ' 't' 'h' 'e' ' ' 'r' 'e' 'm' 'n' 'a']\n",
      "predic:  [' ' 't' 'h' 'e' ' ' 'r' 'e' 'n' 'e' 'e']\n",
      "==================================================\n",
      "target:  ['r' 's' ' ' 'o' 'f' ' ' 't' 'h' 'e' ' ']\n",
      "predic:  ['r' 's' ' ' 'o' 'f' ' ' 't' 'h' 'e' ' ']\n",
      "==================================================\n",
      "Average loss at step 44000: 0.757063 learning_rate 3.972112\n",
      "Minibatch perplexity: 2.11\n",
      "Average loss at step 44200: 0.772583 learning_rate 3.972112\n",
      "Minibatch perplexity: 1.95\n",
      "Average loss at step 44400: 0.766934 learning_rate 3.972112\n",
      "Minibatch perplexity: 2.32\n",
      "==================================================\n",
      "target:  [' ' 'i' 'n' 't' 'o' ' ' 'f' 'o' 'u' 'r']\n",
      "predic:  [' ' 'i' 'n' 't' 'o' ' ' 'b' 'o' 'r' 'r']\n",
      "==================================================\n",
      "target:  [' ' 'n' 'i' 'n' 'e' ' ' 'n' 'i' 'n' 'e']\n",
      "predic:  [' ' 'n' 'i' 'n' 'e' ' ' 'n' 'i' 'n' 'e']\n",
      "==================================================\n",
      "Average loss at step 44600: 0.756334 learning_rate 3.972112\n",
      "Minibatch perplexity: 2.05\n",
      "Average loss at step 44800: 0.770286 learning_rate 3.972112\n",
      "Minibatch perplexity: 2.20\n",
      "Average loss at step 45000: 0.757604 learning_rate 3.539152\n",
      "Minibatch perplexity: 2.08\n",
      "==================================================\n",
      "target:  ['e' 'l' 'e' 'c' 't' 'i' 'o' 'n' ' ' 't']\n",
      "predic:  ['e' 'l' 'e' 'c' 't' 'i' 'o' 'n' ' ' 'o']\n",
      "==================================================\n",
      "target:  [' ' 't' 'r' 'a' 'd' 'i' 't' 'i' 'o' 'n']\n",
      "predic:  [' ' 't' 'r' 'a' 'd' 'i' 't' 'i' 'i' 'n']\n",
      "==================================================\n",
      "Average loss at step 45200: 0.757106 learning_rate 3.539152\n",
      "Minibatch perplexity: 2.03\n",
      "Average loss at step 45400: 0.747905 learning_rate 3.539152\n",
      "Minibatch perplexity: 1.91\n",
      "Average loss at step 45600: 0.757217 learning_rate 3.539152\n",
      "Minibatch perplexity: 2.23\n",
      "==================================================\n",
      "target:  ['i' 'e' 's' ' ' 'a' 'c' 'r' 'o' 's' 's']\n",
      "predic:  ['i' 'e' 's' ' ' 'a' 'c' 's' 'o' 's' 's']\n",
      "==================================================\n",
      "target:  ['d' ' ' 's' 'o' 'o' 'n' ' ' 'e' 'v' 'o']\n",
      "predic:  ['d' ' ' 's' 'o' 'o' 'n' ' ' 'o' 'o' ' ']\n",
      "==================================================\n",
      "Average loss at step 45800: 0.732341 learning_rate 3.539152\n",
      "Minibatch perplexity: 2.10\n",
      "Average loss at step 46000: 0.740207 learning_rate 3.539152\n",
      "Minibatch perplexity: 2.25\n",
      "Average loss at step 46200: 0.742762 learning_rate 3.539152\n",
      "Minibatch perplexity: 2.18\n",
      "==================================================\n",
      "target:  ['e' 's' 't' 'e' 'r' 'n' ' ' 'c' 'u' 'l']\n",
      "predic:  ['e' 's' 't' 'e' 'r' ' ' 'c' 'u' 'u' 'u']\n",
      "==================================================\n",
      "target:  ['i' 'n' ' ' 'w' 'h' 'i' 'c' 'h' ' ' 't']\n",
      "predic:  ['i' 'n' ' ' 'w' 'h' 'i' 'c' 'h' ' ' 'i']\n",
      "==================================================\n",
      "Average loss at step 46400: 0.739795 learning_rate 3.539152\n",
      "Minibatch perplexity: 2.20\n",
      "Average loss at step 46600: 0.746746 learning_rate 3.539152\n",
      "Minibatch perplexity: 2.07\n",
      "Average loss at step 46800: 0.747793 learning_rate 3.539152\n",
      "Minibatch perplexity: 2.05\n",
      "==================================================\n",
      "target:  ['f' ' ' 's' 't' 'u' 'd' 'y' ' ' 'l' 'i']\n",
      "predic:  ['f' ' ' 's' 't' 't' ' ' 'y' 'l' 'l' 'l']\n",
      "==================================================\n",
      "target:  ['n' 'i' 'n' 'e' ' ' 's' 'i' 'x' ' ' 'i']\n",
      "predic:  ['n' 'i' 'n' 'e' ' ' 's' 'i' 'x' ' ' 'n']\n",
      "==================================================\n",
      "Average loss at step 47000: 0.743105 learning_rate 3.539152\n",
      "Minibatch perplexity: 2.08\n",
      "Average loss at step 47200: 0.723316 learning_rate 3.539152\n",
      "Minibatch perplexity: 2.25\n",
      "Average loss at step 47400: 0.722815 learning_rate 3.539152\n",
      "Minibatch perplexity: 1.91\n",
      "==================================================\n",
      "target:  ['z' 'a' 't' 'i' 'o' 'n' ' ' 'o' 'f' ' ']\n",
      "predic:  ['z' 'a' 't' 'i' 'o' 'n' ' ' 'o' 'f' ' ']\n",
      "==================================================\n",
      "target:  [' ' 'c' 'o' 'l' 'o' 'r' ' ' 'e' 'e' 'e']\n",
      "predic:  [' ' 'c' 'o' 'l' 'o' 'r' 'r' 'e' 'e' 'e']\n",
      "==================================================\n",
      "Average loss at step 47600: 0.726035 learning_rate 3.539152\n",
      "Minibatch perplexity: 1.97\n",
      "Average loss at step 47800: 0.709586 learning_rate 3.539152\n",
      "Minibatch perplexity: 2.03\n",
      "Average loss at step 48000: 0.702418 learning_rate 3.539152\n",
      "Minibatch perplexity: 1.96\n",
      "==================================================\n",
      "target:  [' ' 't' 'h' 'e' ' ' 'i' 'n' 'q' 'u' 'i']\n",
      "predic:  [' ' 't' 'h' 'e' ' ' 'i' 'n' 'i' 'q' 'e']\n",
      "==================================================\n",
      "target:  ['a' 'r' 'a' 'c' 't' 'e' 'r' 's' ' ' 'i']\n",
      "predic:  ['a' 'r' 'a' 'c' 't' 'e' 'r' 's' ' ' ' ']\n",
      "==================================================\n",
      "Average loss at step 48200: 0.701530 learning_rate 3.539152\n",
      "Minibatch perplexity: 2.00\n",
      "Average loss at step 48400: 0.705306 learning_rate 3.539152\n",
      "Minibatch perplexity: 2.03\n",
      "Average loss at step 48600: 0.699587 learning_rate 3.539152\n",
      "Minibatch perplexity: 2.13\n",
      "==================================================\n",
      "target:  ['i' 'z' 'e' 'd' ' ' 'c' 'r' 'm' ' ' 'i']\n",
      "predic:  ['i' 'z' 'e' 'd' ' ' 'b' 'r' 'r' ' ' ' ']\n",
      "==================================================\n",
      "target:  ['r' ' ' 'h' 'i' 'm' ' ' 'h' 'e' 'r' 's']\n",
      "predic:  ['r' ' ' 'h' 'h' 'm' 'h' ' ' 'h' 'h' 'r']\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 48800: 0.710729 learning_rate 3.539152\n",
      "Minibatch perplexity: 2.12\n",
      "Average loss at step 49000: 0.709565 learning_rate 3.539152\n",
      "Minibatch perplexity: 2.22\n",
      "Average loss at step 49200: 0.699878 learning_rate 3.539152\n",
      "Minibatch perplexity: 1.95\n",
      "==================================================\n",
      "target:  ['a' 'e' 'o' 'l' 'o' 'g' 'i' 'c' 'a' 'l']\n",
      "predic:  ['a' 'e' 'o' 'l' 'i' 'l' 'i' 'c' 'a' 'a']\n",
      "==================================================\n",
      "target:  ['t' 'i' 'm' 'e' ' ' 'a' 'n' 'd' ' ' 'm']\n",
      "predic:  ['t' 'i' 'm' 'e' ' ' 'a' 'n' 'd' ' ' 'm']\n",
      "==================================================\n",
      "Average loss at step 49400: 0.707197 learning_rate 3.539152\n",
      "Minibatch perplexity: 2.06\n",
      "Average loss at step 49600: 0.711153 learning_rate 3.539152\n",
      "Minibatch perplexity: 2.21\n",
      "Average loss at step 49800: 0.733934 learning_rate 3.539152\n",
      "Minibatch perplexity: 2.15\n",
      "==================================================\n",
      "target:  ['e' 'r' 'n' ' ' 'a' 'r' 'c' 'h' 'a' 'e']\n",
      "predic:  ['e' 'r' 'n' ' ' 'a' 'l' 'l' 'l' 'a' 'e']\n",
      "==================================================\n",
      "target:  ['e' 'i' 'g' 'h' 't' ' ' 'e' 'i' 'g' 'h']\n",
      "predic:  ['e' 'i' 'g' 'h' 't' ' ' 'e' 'i' 'g' 'h']\n",
      "==================================================\n",
      "Average loss at step 50000: 0.726284 learning_rate 3.153384\n",
      "Minibatch perplexity: 2.21\n",
      "Average loss at step 50200: 0.715764 learning_rate 3.153384\n",
      "Minibatch perplexity: 2.36\n",
      "Average loss at step 50400: 0.699559 learning_rate 3.153384\n",
      "Minibatch perplexity: 1.97\n",
      "==================================================\n",
      "target:  ['f' ' ' 'l' 'o' 'c' 'a' 'l' ' ' 'h' 'e']\n",
      "predic:  ['f' ' ' 'l' 'o' 'l' 'a' ' ' ' ' ' ' 'l']\n",
      "==================================================\n",
      "target:  ['s' ' ' 'n' 'u' 'm' 'e' 'r' 'o' 'u' 's']\n",
      "predic:  ['s' ' ' 'n' 'u' 'r' 'm' 'r' 'r' 't' 'o']\n",
      "==================================================\n",
      "Average loss at step 50600: 0.684702 learning_rate 3.153384\n",
      "Minibatch perplexity: 1.85\n",
      "Average loss at step 50800: 0.696447 learning_rate 3.153384\n",
      "Minibatch perplexity: 2.05\n",
      "Average loss at step 51000: 0.698961 learning_rate 3.153384\n",
      "Minibatch perplexity: 1.97\n",
      "==================================================\n",
      "target:  ['c' 'e' ' ' 'i' 's' ' ' 'a' ' ' 'b' 'r']\n",
      "predic:  ['c' 'e' ' ' 'i' 's' ' ' 'a' 'b' 'r' 'r']\n",
      "==================================================\n",
      "target:  ['t' ' ' 'b' 'y' ' ' 't' 'h' 'e' ' ' 'g']\n",
      "predic:  ['t' ' ' 'b' 'y' ' ' 'h' 'h' 'g' ' ' 'g']\n",
      "==================================================\n",
      "Average loss at step 51200: 0.711558 learning_rate 3.153384\n",
      "Minibatch perplexity: 1.91\n",
      "Average loss at step 51400: 0.706706 learning_rate 3.153384\n",
      "Minibatch perplexity: 2.02\n",
      "Average loss at step 51600: 0.714558 learning_rate 3.153384\n",
      "Minibatch perplexity: 2.11\n",
      "==================================================\n",
      "target:  ['o' 't' 'e' 'c' 'h' 'n' 'o' 'l' 'o' 'g']\n",
      "predic:  ['o' 't' 'e' 'c' 't' 't' 'o' 'l' 'l' 'l']\n",
      "==================================================\n",
      "target:  [' ' 'f' 'r' 'o' 'm' ' ' 'a' 'n' 'k' 'a']\n",
      "predic:  [' ' 'f' 'r' 'o' 'm' ' ' 'a' 'n' 'd' ' ']\n",
      "==================================================\n",
      "Average loss at step 51800: 0.702808 learning_rate 3.153384\n",
      "Minibatch perplexity: 1.93\n",
      "Average loss at step 52000: 0.691226 learning_rate 3.153384\n",
      "Minibatch perplexity: 1.86\n",
      "Average loss at step 52200: 0.692017 learning_rate 3.153384\n",
      "Minibatch perplexity: 1.86\n",
      "==================================================\n",
      "target:  ['e' 'x' 'p' 'e' 'r' 'i' 'm' 'e' 'n' 't']\n",
      "predic:  ['e' 'x' 'p' 'e' 'r' 'i' 'i' 'e' 'n' 't']\n",
      "==================================================\n",
      "target:  ['t' 'h' 'e' ' ' 'd' 'i' 'f' 'f' 'e' 'r']\n",
      "predic:  ['t' 'h' 'e' ' ' 'd' 'i' 'f' 'f' 'e' 'e']\n",
      "==================================================\n",
      "Average loss at step 52400: 0.683445 learning_rate 3.153384\n",
      "Minibatch perplexity: 1.96\n",
      "Average loss at step 52600: 0.678971 learning_rate 3.153384\n",
      "Minibatch perplexity: 1.84\n",
      "Average loss at step 52800: 0.666239 learning_rate 3.153384\n",
      "Minibatch perplexity: 2.03\n",
      "==================================================\n",
      "target:  ['o' 'f' ' ' 'w' 'e' 's' 't' 'e' 'r' 'n']\n",
      "predic:  ['o' 'f' ' ' 'w' 'e' 's' 't' 'e' 'r' 'n']\n",
      "==================================================\n",
      "target:  ['t' 'e' ' ' 'a' 'r' 't' 'i' 'c' 'l' 'e']\n",
      "predic:  ['t' 'e' ' ' 'a' 'l' 'c' 'i' 'i' 't' 'i']\n",
      "==================================================\n",
      "Average loss at step 53000: 0.684524 learning_rate 3.153384\n",
      "Minibatch perplexity: 2.10\n",
      "Average loss at step 53200: 0.685823 learning_rate 3.153384\n",
      "Minibatch perplexity: 1.93\n",
      "Average loss at step 53400: 0.666373 learning_rate 3.153384\n",
      "Minibatch perplexity: 2.00\n",
      "==================================================\n",
      "target:  ['t' 'h' ' ' 'c' 'e' 'n' 't' 'u' 'r' 'y']\n",
      "predic:  ['t' 'h' ' ' 'c' 'e' 'n' 't' 'u' 'r' 'y']\n",
      "==================================================\n",
      "target:  ['a' 's' ' ' 'p' 'e' 'r' 's' 'o' 'n' 'a']\n",
      "predic:  ['a' 's' ' ' 'p' 'e' 'r' 's' 'o' 'n' 'n']\n",
      "==================================================\n",
      "Average loss at step 53600: 0.666703 learning_rate 3.153384\n",
      "Minibatch perplexity: 1.85\n",
      "Average loss at step 53800: 0.659063 learning_rate 3.153384\n",
      "Minibatch perplexity: 1.96\n",
      "Average loss at step 54000: 0.663189 learning_rate 3.153384\n",
      "Minibatch perplexity: 1.94\n",
      "==================================================\n",
      "target:  ['a' 'r' 'i' 's' 't' 'o' 't' 'l' 'e' ' ']\n",
      "predic:  ['a' 'r' 'i' 's' 't' 'o' 'r' ' ' ' ' 't']\n",
      "==================================================\n",
      "target:  ['o' 'r' 'i' 'g' 'i' 'n' 'a' 'l' ' ' 'c']\n",
      "predic:  ['o' 'r' 'i' 'g' 'i' 'n' 'a' 'l' ' ' 'c']\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-5871dacd2a2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m       \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     _,l,predictions,lr = sess.run(\n\u001b[1;32m---> 17\u001b[1;33m       [optimizer,loss,train_prediction,learning_rate],feed_dict =feed_dict)\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mmean_loss\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0msummary_frequency\u001b[0m \u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rowan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rowan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rowan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rowan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rowan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "summary_frequency = 200\n",
    "t0 = time()\n",
    "\n",
    "log =[]\n",
    "\n",
    "with tf. Session(graph=graph) as sess:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialised\")\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _,l,predictions,lr = sess.run(\n",
    "      [optimizer,loss,train_prediction,learning_rate],feed_dict =feed_dict)\n",
    "    mean_loss +=l\n",
    "    if step%summary_frequency ==0:\n",
    "      if step>0:\n",
    "        mean_loss = mean_loss/summary_frequency\n",
    "        print(\"Average loss at step %d: %f learning_rate %f\"%(step,mean_loss,lr))\n",
    "        labels = np.concatenate(list(batches))\n",
    "        log.append((step,np.exp(logprob(predictions,labels)),mean_loss))\n",
    "        mean_loss = 0        \n",
    "        print(\"Minibatch perplexity: %.2f\"%float(np.exp(logprob(predictions,labels))))        \n",
    "        if step%(summary_frequency*3) == 0:\n",
    "          print('='*50)\n",
    "          for j in range(2):\t\t\t\t\t\n",
    "            prediction_out = np.ndarray(shape = num_unrollings,dtype = 'U1')\n",
    "            target_out = np.ndarray(shape = num_unrollings,dtype = 'U1')            \n",
    "            for k in range(num_unrollings):\n",
    "              ##print('data_type: ',type(np.argmax(predictions[k*64+j])))\n",
    "              prediction_out[k] = id2char(np.argmax(predictions[k*64+j]))\n",
    "              target_out[k] = id2char(np.argmax(labels[k*64+j]))\n",
    "            print(\"target: \", target_out)\n",
    "            print(\"predic: \",prediction_out)\n",
    "            print('='*50)\n",
    "\n",
    "print(\"model run time: %im: %0.1fs\"%((time()-t0)//60,(time()-t0)%60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_steps = 25001\n",
    "\n",
    "learning rate started at 10 and halfed every 5000 (staircase)\n",
    "\n",
    "num_nodes_1 = 256\n",
    "\n",
    "Average loss at step 25000: 1.281596 learning_rate 0.312500\n",
    "\n",
    "Minibatch perplexity: 3.80\n",
    "\n",
    "model run time: 44m: 47.0s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log2=np.array(log)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(np.array(log2[:,0]), np.array(log2[:,1]), 'b-')\n",
    "ax1.set_xlabel(\"step no.\")\n",
    "# Make the y-axis label, ticks and tick labels match the line color.\n",
    "ax1.set_ylabel('perplexity', color='b')\n",
    "ax1.tick_params('y', colors='b')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.array(log2[:,0]), np.array(log2[:,2]), 'r-')\n",
    "ax2.set_ylabel('average loss', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reversed the encoder!!!\n",
    "\n",
    "num_steps = 11001\n",
    "\n",
    "learning rate started at 10 and *0.3 every 5000 (staircase)\n",
    "\n",
    "num_nodes_1 = 128\n",
    "\n",
    "Average loss at step 11000: 0.841077 learning_rate 0.900000\n",
    "\n",
    "Minibatch perplexity: 2.19\n",
    "\n",
    "model run time: 8m: 50.1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log2=np.array(log)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(np.array(log2[:,0]), np.array(log2[:,1]), 'b-')\n",
    "ax1.set_xlabel(\"step no.\")\n",
    "# Make the y-axis label, ticks and tick labels match the line color.\n",
    "ax1.set_ylabel('perplexity', color='b')\n",
    "ax1.tick_params('y', colors='b')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.array(log2[:,0]), np.array(log2[:,2]), 'r-')\n",
    "ax2.set_ylabel('average loss', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reversed the encoder!!! -- The output for this model was deleted!!! a little better than the smaller model\n",
    "\n",
    "num_steps = 11001\n",
    "\n",
    "learning rate started at 10 and *0.55 every 5000 (staircase)\n",
    "\n",
    "num_nodes_1 = 256\n",
    "\n",
    "Average loss at step 25000: 0.779838 learning_rate 0.503284\n",
    "\n",
    "Minibatch perplexity: 2.07\n",
    "\n",
    "model run time: 44m: 59.6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log2=np.array(log)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(np.array(log2[:,0]), np.array(log2[:,1]), 'b-')\n",
    "ax1.set_xlabel(\"step no.\")\n",
    "# Make the y-axis label, ticks and tick labels match the line color.\n",
    "ax1.set_ylabel('perplexity', color='b')\n",
    "ax1.tick_params('y', colors='b')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(np.array(log2[:,0]), np.array(log2[:,2]), 'r-')\n",
    "ax2.set_ylabel('average loss', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
